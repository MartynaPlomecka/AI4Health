{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mat73\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "seed_value = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = ''\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    if \"anuja\" in os.environ.get('USER'):\n",
    "        DATA_DIR = 'data/'\n",
    "    elif 'ubuntu' in os.environ.get('USER'):\n",
    "        DATA_DIR = '/home/ubuntu/Martyna/repo/AI4Health/DATAfoof/'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foof = pd.read_csv(DATA_DIR+\"source_intercept_slope.csv\")\n",
    "# # #foof = foof.rename(columns={\"C1\": \"IDs\" ,\"C2\": \"Intercept\", \"C3\": \"Slope\"})\n",
    "# foof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = mat73.loadmat(DATA_DIR+'x_source_new.mat')  \n",
    "# df2 = pd.DataFrame(data['x'].reshape((data['x'].shape[0], -1)))\n",
    "\n",
    "# # sparsing\n",
    "# df2 = np.array(df2).reshape(data['x'].shape) \n",
    "# df2_sparsed = np.concatenate([np.expand_dims(df2[:,:,i:i+2].mean(axis = 2), axis = 2) for i in range(0, data['x'].shape[2]-2, 2)], axis = 2)\n",
    "# df2 = pd.DataFrame(df2_sparsed.reshape((df2_sparsed.shape[0], -1)))\n",
    "\n",
    "# #scaling\n",
    "# norm = MinMaxScaler().fit(df2)\n",
    "# df2 = norm.transform(df2)\n",
    "# df2 = pd.DataFrame(df2.reshape((df2.shape[0], -1)))\n",
    "\n",
    "# df2['IDs'] = foof['IDs']\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beh = pd.read_csv(DATA_DIR+\"behaviorals.csv\")\n",
    "# print('Before:'+str(beh.shape))\n",
    "\n",
    "# most_common_disorders = ['Attention-Deficit/Hyperactivity Disorder', 'Anxiety Disorders', 'Specific Learning Disorder',\n",
    "#                          'Autism Spectrum Disorder', 'Disruptive', 'Communication Disorder',\n",
    "#                          'Depressive Disorders',  'No Diagnosis Given', 'Other Disorders']\n",
    "\n",
    "# category_columns = ['DX_' + str(i).zfill(2) + '_Cat' for i in range(1, 11)] +\\\n",
    "#                    ['DX_' + str(i).zfill(2) + '_Sub' for i in range(1, 11)]\n",
    "\n",
    "# # removing patients with incomplete eval\n",
    "# initial_size = beh.shape[0]\n",
    "# beh = beh[beh.DX_01 != 'No Diagnosis Given: Incomplete Eval']\n",
    "# beh = beh.reset_index(drop=True)\n",
    "# new_size = beh.shape[0]\n",
    "\n",
    "# print('After:'+str(beh.shape))\n",
    "# print('Removing', initial_size - new_size,\n",
    "#       'patients as their evaluations was incomplete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_diagnosis_given = 'No Diagnosis Given'\n",
    "\n",
    "# diagnoses_to_ids = {disorder: i for i, disorder in enumerate(most_common_disorders)}\n",
    "# diagnoses_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_disorder(data, row, index):\n",
    "#     disorder = data.iloc[row][category_columns[index]]\n",
    "\n",
    "#     if disorder == 'Neurodevelopmental Disorders':\n",
    "#         disorder = data.iloc[row][category_columns[index + 10]]\n",
    "\n",
    "#     return disorder\n",
    "\n",
    "# order_of_disorders = []\n",
    "# for k in range(beh.shape[0]):\n",
    "#     i = 0\n",
    "#     disorder = get_disorder(beh, k, i)\n",
    "#     disorders_patient = []\n",
    "#     while not pd.isnull(disorder):\n",
    "#         if disorder in diagnoses_to_ids:\n",
    "#             if diagnoses_to_ids[disorder] not in disorders_patient:\n",
    "#                 disorders_patient.append(diagnoses_to_ids[disorder])\n",
    "#         else:\n",
    "#             if diagnoses_to_ids['Other Disorders'] not in disorders_patient:\n",
    "#                 disorders_patient.append(diagnoses_to_ids['Other Disorders'])\n",
    "#         i += 1\n",
    "#         if i == len(category_columns):\n",
    "#             break\n",
    "#         disorder = get_disorder(beh, k, i)\n",
    "\n",
    "        \n",
    "#     order_of_disorders.append(disorders_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_disorders = []\n",
    "# no_diagnosis_given = []\n",
    "# for i in order_of_disorders:\n",
    "#     if 7 in i:\n",
    "#         no_diagnosis_given.append(1)\n",
    "#         i.remove(7)\n",
    "#     else:\n",
    "#         no_diagnosis_given.append(0)\n",
    "#     if 8 in i:\n",
    "#         other_disorders.append(1)\n",
    "#         i.remove(8)\n",
    "#     else:\n",
    "#         other_disorders.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len_order = np.max([len(x) for x in order_of_disorders])\n",
    "\n",
    "# # pad with a new token denoting the pad token\n",
    "# pad_token = len(most_common_disorders)\n",
    "# bod_token = len(most_common_disorders) + 1\n",
    "# eod_token = len(most_common_disorders) + 2\n",
    "\n",
    "# order_of_disorders = [[bod_token] + x + [eod_token] + [pad_token] * (max_len_order - len(x)) for x in order_of_disorders]\n",
    "\n",
    "# order_of_disorders = np.array(order_of_disorders)\n",
    "\n",
    "# classes = np.zeros((len(most_common_disorders),\n",
    "#                     beh.shape[0]), dtype=np.int32)\n",
    "\n",
    "# df_disorders = beh[category_columns]\n",
    "\n",
    "# for i, disorder in enumerate(most_common_disorders):\n",
    "#     mask = df_disorders.select_dtypes(include=[object]). \\\n",
    "#         applymap(lambda x: disorder in x if pd.notnull(x) else False)\n",
    "\n",
    "#     disorder_df = df_disorders[mask.any(axis=1)]\n",
    "\n",
    "#     np.add.at(classes[i], disorder_df.index.values, 1)\n",
    "\n",
    "# behaviour_data_columns = beh.columns.values.astype(np.str)\n",
    "\n",
    "# columns_to_drop = behaviour_data_columns[\n",
    "#     np.flatnonzero(np.core.defchararray.find(behaviour_data_columns, 'DX') != -1)]\n",
    "\n",
    "# behaviour_data = beh.drop(columns=columns_to_drop)\n",
    "\n",
    "# for disorder, classification in zip(most_common_disorders, classes):\n",
    "#     behaviour_data[disorder] = classification\n",
    "\n",
    "# behaviour_data['order_diagnoses'] = list(order_of_disorders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_disorders = ['Attention-Deficit/Hyperactivity Disorder', 'Anxiety Disorders', 'Specific Learning Disorder',\n",
    "#                          'Autism Spectrum Disorder', 'Disruptive', 'Communication Disorder',\n",
    "#                          'Depressive Disorders']\n",
    "\n",
    "# labels=behaviour_data[[\"IDs\"]+list(common_disorders)]\n",
    "# labels[\"Other Disorders\"] = other_disorders\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_features = behaviour_data[['Sex', 'Age','IDs']]\n",
    "# add_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.merge(df2, foof, on='IDs', how='inner')\n",
    "# df = pd.merge(df2, labels, on='IDs', how='inner')\n",
    "# df = pd.merge(df, add_features, on='IDs', how='inner')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disorders_list = ['Attention-Deficit/Hyperactivity Disorder', 'Anxiety Disorders', 'Specific Learning Disorder',\n",
    "#                       'Autism Spectrum Disorder', 'Disruptive', 'Communication Disorder',\n",
    "#                       'Depressive Disorders', 'Other Disorders']\n",
    "# x = df[df.columns.difference(['IDs']+disorders_list)]\n",
    "# y = df[disorders_list]\n",
    "\n",
    "# # # summarize dataset shape\n",
    "# print(x.shape, y.shape)\n",
    "\n",
    "# train_features, test_features, train_labels, test_labels = train_test_split(x, y, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(DATA_DIR, 'multilabel_classification_source_level_age_gender')\n",
    "\n",
    "# # #saving\n",
    "# if not(os.path.exists(path)):\n",
    "#      os.mkdir(path)\n",
    "# np.save(os.path.join(path, 'train_features.npy'), train_features)\n",
    "# np.save(os.path.join(path, 'test_features.npy'), test_features)\n",
    "# np.save(os.path.join(path, 'train_labels.npy'), train_labels)\n",
    "# np.save(os.path.join(path, 'test_labels.npy'), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading\n",
    "train_features = np.load(os.path.join(path, 'train_features.npy'))\n",
    "test_features = np.load(os.path.join(path, 'test_features.npy'))\n",
    "train_labels = np.load(os.path.join(path, 'train_labels.npy'))\n",
    "test_labels = np.load(os.path.join(path, 'test_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np. array(train_labels) \n",
    "test_labels = np. array(test_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE EXTRACTION WITH PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_gender = True\n",
    "if age_gender:\n",
    "    train_age_gender = train_features[:,-2:]\n",
    "    test_age_gender = test_features[:,-2:]\n",
    "    train_features = train_features[:,:-2]\n",
    "    test_features = test_features[:,:-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 2652) (481, 2652)\n"
     ]
    }
   ],
   "source": [
    "#scaling features\n",
    "\n",
    "# data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(train_features)\n",
    "\n",
    "# transform training data\n",
    "train_features = norm.transform(train_features)\n",
    "test_features = norm.transform(test_features)\n",
    "\n",
    "print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 407) (481, 407)\n"
     ]
    }
   ],
   "source": [
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(.95) # 95% variance retained\n",
    "pca.fit(train_features)\n",
    "\n",
    "# transform data\n",
    "train_features = pca.transform(train_features)\n",
    "test_features = pca.transform(test_features)\n",
    "print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if age_gender:\n",
    "    train_features = np.concatenate([train_features, train_age_gender], axis = 1)\n",
    "    test_features = np.concatenate([test_features, test_age_gender], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE EXTRACTION WITH AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model, load_model\n",
    "# from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization\n",
    "# from tensorflow.keras.utils import plot_model, to_categorical\n",
    "\n",
    "# n_inputs = train_features.shape[1]\n",
    "\n",
    "# visible = Input(shape=(n_inputs,))\n",
    "# e = Dense(n_inputs*2)(visible)\n",
    "# e = BatchNormalization()(e)\n",
    "# e = LeakyReLU()(e)\n",
    "# e = Dense(n_inputs)(e)\n",
    "# e = BatchNormalization()(e)\n",
    "# e = LeakyReLU()(e)\n",
    "# n_bottleneck = round(float(n_inputs) / 2.0)\n",
    "# bottleneck = Dense(n_bottleneck)(e)\n",
    "\n",
    "# d = Dense(n_inputs)(bottleneck)\n",
    "# d = BatchNormalization()(d)\n",
    "# d = LeakyReLU()(d)\n",
    "\n",
    "# d = Dense(n_inputs*2)(d)\n",
    "# d = BatchNormalization()(d)\n",
    "# d = LeakyReLU()(d)\n",
    "\n",
    "# output = Dense(8, activation='linear')(d)\n",
    "\n",
    "# model = Model(inputs=visible, outputs=output)\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# plot_model(model, 'autoencoder_compress.png', show_shapes=True)\n",
    "\n",
    "# history = model.fit(train_features, train_labels, epochs=50, batch_size=16, verbose=2)\n",
    "# encoder = Model(inputs=visible, outputs=bottleneck)\n",
    "\n",
    "# plot_model(encoder, 'encoder_compress.png', show_shapes=True)\n",
    "# encoder.save('autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode the data\n",
    "# encoder= load_model('autoencoder.h5', compile=False)\n",
    "\n",
    "# train_features = encoder.predict(train_features)\n",
    "# test_features = encoder.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, PrecisionRecallDisplay\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "n_classes = train_labels.shape[1]\n",
    "\n",
    "def plot_pr_curve(Y_test, y_score):\n",
    "    # For each class\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict() #actually stores AUC\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i], y_score[:, i])\n",
    "        average_precision[i] = auc(recall[i], precision[i])\n",
    "        average_precision[i] = auc(Y_test[:, i], y_score[:, i])\n",
    "\n",
    "    # A \"micro-average\": quantifying score on all classes jointly\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
    "        Y_test.ravel(), y_score.ravel()\n",
    "    )\n",
    "#     average_precision[\"macro\"] = auc(recall[\"micro\"], precision[\"micro\"])\n",
    "    average_precision[\"macro\"] = np.average(list(average_precision.values()))\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines, labels = [], []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[\"micro\"],\n",
    "        precision=precision[\"micro\"],\n",
    "        average_precision=average_precision[\"macro\"],\n",
    "    )\n",
    "    display.plot(ax=ax, name=\"Micro-average precision-recall\", color=\"gold\")\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        display = PrecisionRecallDisplay(\n",
    "            recall=recall[i],\n",
    "            precision=precision[i],\n",
    "            average_precision=average_precision[i],\n",
    "        )\n",
    "        display.plot(ax=ax, name=f\"Precision-recall for class {i}\")\n",
    "\n",
    "    # add the legend for the iso-f1 curves\n",
    "    handles, labels = display.ax_.get_legend_handles_labels()\n",
    "    handles.extend([l])\n",
    "    labels.extend([\"iso-f1 curves\"])\n",
    "    # set the legend and the axes\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.legend(handles=handles, labels=labels, loc=\"best\")\n",
    "    ax.set_title(\"Precision-Recall curve\")\n",
    "    plt.show()\n",
    "    \n",
    "    return average_precision[\"macro\"]\n",
    "    \n",
    "def plot_roc_curve(y_test, y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "   \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "#     roc_auc[\"macro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    roc_auc[\"macro\"] = np.average(list(roc_auc.values()))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]))\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                       ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operator characteristics')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc[\"macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def brier_multi(targets, probs):\n",
    "    return np.mean(np.sum((probs - targets)**2, axis=1))\n",
    "\n",
    "def evaluate(y_test, y_pred_prob, brier=True):\n",
    "    y_pred = y_pred_prob.round()\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    hamming = hamming_loss(y_test, y_pred)\n",
    "    brier_score = brier_multi(y_test, y_pred_prob)\n",
    "    report = skm.classification_report(y_test,y_pred, zero_division=1, output_dict=True)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Hamming Loss:\", hamming)\n",
    "    if brier:\n",
    "        print(\"Brier Score:\", brier_score)\n",
    "    print(\"Classification Report:\\n\", skm.classification_report(y_test,y_pred, zero_division=1))\n",
    "    print(\"Confusion matrix:\\n\", skm.multilabel_confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return accuracy,  hamming, brier_score, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "accuracy_scores, hamming, brier_score, precision, recall, f1_score, pr_auc, roc_auc = [], [], [], [], [], [], [], []\n",
    "accuracy_scores_all, hamming_all, brier_score_all, precision_all, recall_all, f1_score_all, pr_auc_all, roc_auc_all = [], [], [], [], [], [], [], []\n",
    "accuracy_scores_std, hamming_std, brier_score_std, precision_std, recall_std, f1_score_std, pr_auc_std, roc_auc_std = [], [], [], [], [], [], [], []\n",
    "\n",
    "def record_values(a, h, b, p, r, f, pr, roc,a_std, h_std, b_std, p_std, r_std, f_std, pr_std, roc_std, model_name=''):\n",
    "    accuracy_scores.append(a)\n",
    "    hamming.append(h)\n",
    "    brier_score.append(b)\n",
    "    precision.append(p)\n",
    "    recall.append(r)\n",
    "    f1_score.append(f)\n",
    "    pr_auc.append(pr)\n",
    "    roc_auc.append(roc)\n",
    "    \n",
    "    accuracy_scores_std.append(a_std)\n",
    "    hamming_std.append(h_std)\n",
    "    brier_score_std.append(b_std)\n",
    "    precision_std.append(p_std)\n",
    "    recall_std.append(r_std)\n",
    "    f1_score_std.append(f_std)\n",
    "    pr_auc_std.append(pr_std)\n",
    "    roc_auc_std.append(roc_std)\n",
    "    model_names.append(model_name)\n",
    "    \n",
    "\n",
    "def record_values_all(a, h, b, p, r, f, pr, roc, model_name=''):\n",
    "    accuracy_scores_all.append(a)\n",
    "    hamming_all.append(h)\n",
    "    brier_score_all.append(b)\n",
    "    precision_all.append(p)\n",
    "    recall_all.append(r)\n",
    "    f1_score_all.append(f)\n",
    "    pr_auc_all.append(pr)\n",
    "    roc_auc_all.append(roc)\n",
    "    model_names_all.append(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci(vals):\n",
    "    return np.percentile(vals, 5), np.percentile(vals, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mit shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import sys, os\n",
    "       \n",
    "a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(n):\n",
    "    predicted_labels_shuffled = np.copy(test_labels)\n",
    "    np.random.shuffle(predicted_labels_shuffled)\n",
    "    accuracy_, hamming_, brier_score_, report_ = evaluate(test_labels, predicted_labels_shuffled)\n",
    "    pr = plot_pr_curve(test_labels, predicted_labels_shuffled)\n",
    "    roc = plot_roc_curve(test_labels, predicted_labels_shuffled)\n",
    "    \n",
    "    a_avg.append(accuracy_)\n",
    "    h_avg.append(hamming_)\n",
    "    b_avg.append(brier_score_)\n",
    "    p_avg.append(report_['micro avg']['precision'])\n",
    "    r_avg.append(report_['micro avg']['recall'])\n",
    "    f_avg.append(report_['micro avg']['f1-score'])\n",
    "    pr_avg.append(pr)\n",
    "    roc_avg.append(roc)\n",
    "    record_values_all(accuracy_, hamming_, brier_score_, report_['micro avg']['precision'], report_['micro avg']['recall'], report_['micro avg']['f1-score'], pr, roc, 'Shuffling Baseline')\n",
    "\n",
    "\n",
    "vals = [a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg]\n",
    "means = [np.mean(v) for v in vals]\n",
    "cis = [ci(v) for v in vals]\n",
    "record_values(*means, *cis, 'Shuffling Baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dummy classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%capture\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "n=10\n",
    "strategies = ['stratified', 'most_frequent', 'uniform']\n",
    "\n",
    "for strategy_ in strategies:\n",
    "\n",
    "    a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "    for i in range(n):\n",
    "        dummy_clf = DummyClassifier(strategy=strategy_)\n",
    "        dummy_clf.fit(train_features, train_labels)\n",
    "        predicted_labels = np.asarray(dummy_clf.predict_proba(test_features))[:, :, 1].T\n",
    "        print('Dummy Classifier-' + strategy_)\n",
    "        evaluate(test_labels, predicted_labels)\n",
    "        accuracy_, hamming_, brier_score_, report_ = evaluate(test_labels, predicted_labels)\n",
    "        pr = plot_pr_curve(test_labels, predicted_labels)\n",
    "        roc = plot_roc_curve(test_labels, predicted_labels)\n",
    "        \n",
    "        a_avg.append(accuracy_)\n",
    "        h_avg.append(hamming_)\n",
    "        b_avg.append(brier_score_)\n",
    "        p_avg.append(report_['micro avg']['precision'])\n",
    "        r_avg.append(report_['micro avg']['recall'])\n",
    "        f_avg.append(report_['micro avg']['f1-score'])\n",
    "        pr_avg.append(pr)\n",
    "        roc_avg.append(roc)\n",
    "\n",
    "    record_values(np.mean(a_avg),np.mean(h_avg), np.mean(b_avg), np.mean(p_avg), np.mean(r_avg), np.mean(f_avg), np.mean(pr_avg), np.mean(roc_avg), np.std(a_avg),np.std(h_avg), np.std(b_avg), np.std(p_avg), np.std(r_avg), np.std(f_avg), np.std(pr_avg), np.std(roc_avg), 'Dummy Classifier-' + strategy_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strategies = ['stratified', 'most_frequent', 'uniform']\n",
    "\n",
    "for strategy_ in strategies:\n",
    "\n",
    "    a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "    for i in range(n):\n",
    "        dummy_clf = DummyClassifier(strategy=strategy_)\n",
    "        dummy_clf.fit(train_features, train_labels)\n",
    "        predicted_labels = np.asarray(dummy_clf.predict_proba(test_features))[:, :, 1].T\n",
    "        print('Dummy Classifier-' + strategy_)\n",
    "        evaluate(test_labels, predicted_labels)\n",
    "        accuracy_, hamming_, brier_score_, report_ = evaluate(test_labels, predicted_labels)\n",
    "        pr = plot_pr_curve(test_labels, predicted_labels)\n",
    "        roc = plot_roc_curve(test_labels, predicted_labels)\n",
    "        \n",
    "        a_avg.append(accuracy_)\n",
    "        h_avg.append(hamming_)\n",
    "        b_avg.append(brier_score_)\n",
    "        p_avg.append(report_['micro avg']['precision'])\n",
    "        r_avg.append(report_['micro avg']['recall'])\n",
    "        f_avg.append(report_['micro avg']['f1-score'])\n",
    "        pr_avg.append(pr)\n",
    "        roc_avg.append(roc)\n",
    "\n",
    "    record_values(np.mean(a_avg),np.mean(h_avg), np.mean(b_avg), np.mean(p_avg), np.mean(r_avg), np.mean(f_avg), np.mean(pr_avg), np.mean(roc_avg), np.std(a_avg),np.std(h_avg), np.std(b_avg), np.std(p_avg), np.std(r_avg), np.std(f_avg), np.std(pr_avg), np.std(roc_avg), 'Dummy Classifier-' + strategy_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "    for i in range(n):\n",
    "        dummy_clf = DummyClassifier(strategy=strategy_)\n",
    "        dummy_clf.fit(train_features, train_labels)\n",
    "        predicted_labels = np.asarray(dummy_clf.predict_proba(test_features))[:, :, 1].T\n",
    "        print('Dummy Classifier-' + strategy_)\n",
    "        evaluate(test_labels, predicted_labels)\n",
    "        accuracy_, hamming_, brier_score_, report_ = evaluate(test_labels, predicted_labels)\n",
    "        pr = plot_pr_curve(test_labels, predicted_labels)\n",
    "        roc = plot_roc_curve(test_labels, predicted_labels)\n",
    "        \n",
    "        a_avg.append(accuracy_)\n",
    "        h_avg.append(hamming_)\n",
    "        b_avg.append(brier_score_)\n",
    "        p_avg.append(report_['micro avg']['precision'])\n",
    "        r_avg.append(report_['micro avg']['recall'])\n",
    "        f_avg.append(report_['micro avg']['f1-score'])\n",
    "        pr_avg.append(pr)\n",
    "        roc_avg.append(roc)\n",
    "\n",
    "    record_values(np.mean(a_avg),np.mean(h_avg), np.mean(b_avg), np.mean(p_avg), np.mean(r_avg), np.mean(f_avg), np.mean(pr_avg), np.mean(roc_avg), np.std(a_avg),np.std(h_avg), np.std(b_avg), np.std(p_avg), np.std(r_avg), np.std(f_avg), np.std(pr_avg), np.std(roc_avg), 'Dummy Classifier-' + strategy_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "    for i in range(n):\n",
    "        dummy_clf = DummyClassifier(strategy=strategy_)\n",
    "        dummy_clf.fit(train_features, train_labels)\n",
    "        predicted_labels = np.asarray(dummy_clf.predict_proba(test_features))[:, :, 1].T\n",
    "        print('Dummy Classifier-' + strategy_)\n",
    "        evaluate(test_labels, predicted_labels)\n",
    "        accuracy_, hamming_, brier_score_, report_ = evaluate(test_labels, predicted_labels)\n",
    "        pr = plot_pr_curve(test_labels, predicted_labels)\n",
    "        roc = plot_roc_curve(test_labels, predicted_labels)\n",
    "        \n",
    "        a_avg.append(accuracy_)\n",
    "        h_avg.append(hamming_)\n",
    "        b_avg.append(brier_score_)\n",
    "        p_avg.append(report_['micro avg']['precision'])\n",
    "        r_avg.append(report_['micro avg']['recall'])\n",
    "        f_avg.append(report_['micro avg']['f1-score'])\n",
    "        pr_avg.append(pr)\n",
    "        roc_avg.append(roc)\n",
    "\n",
    "    record_values(np.mean(a_avg),np.mean(h_avg), np.mean(b_avg), np.mean(p_avg), np.mean(r_avg), np.mean(f_avg), np.mean(pr_avg), np.mean(roc_avg), np.std(a_avg),np.std(h_avg), np.std(b_avg), np.std(p_avg), np.std(r_avg), np.std(f_avg), np.std(pr_avg), np.std(roc_avg), 'Dummy Classifier-' + strategy_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Output Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import (precision_recall_curve, PrecisionRecallDisplay)\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "lg = LogisticRegression()\n",
    "models = [lg, forest]\n",
    "\n",
    "number_of_bootstraps = 10\n",
    "seed_value = 7\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "    for i in range(n):\n",
    "\n",
    "        multi_output_model = MultiOutputClassifier(model, n_jobs=-1)\n",
    "        multi_output_model.fit(train_features, train_labels)\n",
    "\n",
    "        # Boostrap\n",
    "        resampled_xtest, resampled_ytest = resample(test_features, test_labels, replace=True, n_samples=len(test_features), random_state=seed_value+i)\n",
    "\n",
    "        predicted_labels = np.asarray(multi_output_model.predict_proba(resampled_xtest))[:, :, 1].T\n",
    "        print(str(model)+':')\n",
    "        accuracy_, hamming_, brier_score_, report_ = evaluate(resampled_ytest, predicted_labels)\n",
    "        pr = plot_pr_curve(resampled_ytest, predicted_labels)\n",
    "        roc = plot_roc_curve(resampled_ytest, predicted_labels)\n",
    "        \n",
    "        a_avg.append(accuracy_)\n",
    "        h_avg.append(hamming_)\n",
    "        b_avg.append(brier_score_)\n",
    "        p_avg.append(report_['micro avg']['precision'])\n",
    "        r_avg.append(report_['micro avg']['recall'])\n",
    "        f_avg.append(report_['micro avg']['f1-score'])\n",
    "        pr_avg.append(pr)\n",
    "        roc_avg.append(roc)\n",
    "        record_values_all(accuracy_, hamming_, brier_score_, report_['micro avg']['precision'], report_['micro avg']['recall'], report_['micro avg']['f1-score'], pr, roc, 'Multi Output Classifier-'+str(model))\n",
    "\n",
    "    \n",
    "    vals = [a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg]\n",
    "    means = [np.mean(v) for v in vals]\n",
    "    cis = [ci(v) for v in vals]\n",
    "    record_values(*means, *cis, 'Multi Output Classifier-'+str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(a_avg), np.mean(h_avg), np.mean(b_avg), np.mean(p_avg), np.mean(r_avg), np.mean(f_avg), np.mean(pr_avg), np.mean(roc_avg), np.std(a_avg), np.std(h_avg), np.std(b_avg), np.std(p_avg), np.std(r_avg), np.std(f_avg), np.std(pr_avg), np.std(roc_avg))\n",
    "accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP SIMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def get_mlp(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(n):\n",
    "    n_inputs, n_outputs = train_features.shape[1],train_labels.shape[1]\n",
    "    mlp = get_mlp(n_inputs, n_outputs)\n",
    "    mlp.fit(train_features, train_labels, verbose=0, epochs=100)\n",
    "\n",
    "    # Boostrap\n",
    "    resampled_xtest, resampled_ytest = resample(test_features, test_labels, replace=True, n_samples=len(test_features), random_state=seed_value+i)\n",
    "\n",
    "    predicted_labels_mlp = mlp.predict(resampled_xtest)\n",
    "    accuracy_, hamming_, brier_score_, report_ = evaluate(resampled_ytest, predicted_labels_mlp)\n",
    "\n",
    "    pr = plot_pr_curve(resampled_ytest, predicted_labels_mlp)\n",
    "    roc = plot_roc_curve(resampled_ytest, predicted_labels_mlp)\n",
    "    \n",
    "    a_avg.append(accuracy_)\n",
    "    h_avg.append(hamming_)\n",
    "    b_avg.append(brier_score_)\n",
    "    p_avg.append(report_['micro avg']['precision'])\n",
    "    r_avg.append(report_['micro avg']['recall'])\n",
    "    f_avg.append(report_['micro avg']['f1-score'])\n",
    "    pr_avg.append(pr)\n",
    "    roc_avg.append(roc)\n",
    "    record_values_all(accuracy_, hamming_, brier_score_, report_['micro avg']['precision'], report_['micro avg']['recall'], report_['micro avg']['f1-score'], pr, roc, 'MLP-1L')\n",
    "\n",
    "\n",
    "vals = [a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg]\n",
    "means = [np.mean(v) for v in vals]\n",
    "cis = [ci(v) for v in vals]\n",
    "record_values(*means, *cis, 'MLP-1L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def get_mlp(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(16, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(16, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(32, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(n):\n",
    "    n_inputs, n_outputs = train_features.shape[1],train_labels.shape[1]\n",
    "    mlp = get_mlp(n_inputs, n_outputs)\n",
    "    mlp.fit(train_features, train_labels, verbose=0, epochs=100)\n",
    "\n",
    "    # Boostrap\n",
    "    resampled_xtest, resampled_ytest = resample(test_features, test_labels, replace=True, n_samples=len(test_features), random_state=seed_value+i)\n",
    "\n",
    "    predicted_labels_mlp = mlp.predict(resampled_xtest)\n",
    "    accuracy_, hamming_, brier_score_, report_ = evaluate(resampled_ytest, predicted_labels_mlp)\n",
    "\n",
    "    pr = plot_pr_curve(resampled_ytest, predicted_labels_mlp)\n",
    "    roc = plot_roc_curve(resampled_ytest, predicted_labels_mlp)\n",
    "    \n",
    "    a_avg.append(accuracy_)\n",
    "    h_avg.append(hamming_)\n",
    "    b_avg.append(brier_score_)\n",
    "    p_avg.append(report_['micro avg']['precision'])\n",
    "    r_avg.append(report_['micro avg']['recall'])\n",
    "    f_avg.append(report_['micro avg']['f1-score'])\n",
    "    pr_avg.append(pr)\n",
    "    roc_avg.append(roc)\n",
    "    record_values_all(accuracy_, hamming_, brier_score_, report_['micro avg']['precision'], report_['micro avg']['recall'], report_['micro avg']['f1-score'], pr, roc, 'MLP-compl')\n",
    "\n",
    "\n",
    "vals = [a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg]\n",
    "means = [np.mean(v) for v in vals]\n",
    "cis = [ci(v) for v in vals]\n",
    "record_values(*means, *cis, 'MLP-compl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Relevance\n",
    "ignores the possible correlations between class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(n):\n",
    "    classifier = BinaryRelevance(GaussianNB())\n",
    "    classifier.fit(train_features, train_labels)\n",
    "\n",
    "    # Boostrap\n",
    "    resampled_xtest, resampled_ytest = resample(test_features, test_labels, replace=True, n_samples=len(test_features), random_state=seed_value+i)\n",
    "\n",
    "\n",
    "    predicted_labels_br = classifier.predict_proba(resampled_xtest)\n",
    "    accuracy_, hamming_, brier_score_, report_ = evaluate(resampled_ytest, predicted_labels_br.toarray())\n",
    "    \n",
    "    pr = plot_pr_curve(resampled_ytest, predicted_labels_br.toarray())\n",
    "    roc = plot_roc_curve(resampled_ytest, predicted_labels_br.toarray())\n",
    "    \n",
    "    a_avg.append(accuracy_)\n",
    "    h_avg.append(hamming_)\n",
    "    b_avg.append(brier_score_)\n",
    "    p_avg.append(report_['micro avg']['precision'])\n",
    "    r_avg.append(report_['micro avg']['recall'])\n",
    "    f_avg.append(report_['micro avg']['f1-score'])\n",
    "    pr_avg.append(pr)\n",
    "    roc_avg.append(roc)\n",
    "    record_values_all(accuracy_, hamming_, brier_score_, report_['micro avg']['precision'], report_['micro avg']['recall'], report_['micro avg']['f1-score'], pr, roc, 'Binary relevance')\n",
    "\n",
    "    \n",
    "\n",
    "vals = [a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg]\n",
    "means = [np.mean(v) for v in vals]\n",
    "cis = [ci(v) for v in vals]\n",
    "record_values(*means, *cis, 'Binary relevance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classfier Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(n):\n",
    "    classifier = ClassifierChain(LogisticRegression())\n",
    "    classifier.fit(train_features, train_labels)\n",
    "\n",
    "    # Boostrap\n",
    "    resampled_xtest, resampled_ytest = resample(test_features, test_labels, replace=True, n_samples=len(test_features), random_state=seed_value+i)\n",
    "\n",
    "    predicted_labels_cc = classifier.predict_proba(resampled_xtest)\n",
    "    accuracy_, hamming_, brier_score_, report_ = evaluate(resampled_ytest, predicted_labels_cc.toarray())\n",
    "    pr = plot_pr_curve(resampled_ytest, predicted_labels_cc.toarray())\n",
    "    roc = plot_roc_curve(resampled_ytest, predicted_labels_cc.toarray())\n",
    "    \n",
    "    a_avg.append(accuracy_)\n",
    "    h_avg.append(hamming_)\n",
    "    b_avg.append(brier_score_)\n",
    "    p_avg.append(report_['micro avg']['precision'])\n",
    "    r_avg.append(report_['micro avg']['recall'])\n",
    "    f_avg.append(report_['micro avg']['f1-score'])\n",
    "    pr_avg.append(pr)\n",
    "    roc_avg.append(roc)\n",
    "    record_values_all(accuracy_, hamming_, brier_score_, report_['micro avg']['precision'], report_['micro avg']['recall'], report_['micro avg']['f1-score'], pr, roc, 'Classifier Chains')\n",
    "\n",
    "\n",
    "vals = [a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg]\n",
    "means = [np.mean(v) for v in vals]\n",
    "cis = [ci(v) for v in vals]\n",
    "record_values(*means, *cis, 'Classifier Chains')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Powerset\n",
    "takes correlations into account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(n):\n",
    "    classifier = LabelPowerset(LogisticRegression())\n",
    "    classifier.fit(train_features, train_labels)\n",
    "\n",
    "    # Boostrap\n",
    "    resampled_xtest, resampled_ytest = resample(test_features, test_labels, replace=True, n_samples=len(test_features), random_state=seed_value+i)\n",
    "\n",
    "    \n",
    "    predicted_labels_lp = classifier.predict_proba(resampled_xtest)\n",
    "    accuracy_, hamming_, brier_score_, report_ = evaluate(resampled_ytest, predicted_labels_lp.toarray())\n",
    "    pr = plot_pr_curve(resampled_ytest, predicted_labels_lp.toarray())\n",
    "    roc = plot_roc_curve(resampled_ytest, predicted_labels_lp.toarray())\n",
    "    \n",
    "    a_avg.append(accuracy_)\n",
    "    h_avg.append(hamming_)\n",
    "    b_avg.append(brier_score_)\n",
    "    p_avg.append(report_['micro avg']['precision'])\n",
    "    r_avg.append(report_['micro avg']['recall'])\n",
    "    f_avg.append(report_['micro avg']['f1-score'])\n",
    "    pr_avg.append(pr)\n",
    "    roc_avg.append(roc)\n",
    "    record_values_all(accuracy_, hamming_, brier_score_, report_['micro avg']['precision'], report_['micro avg']['recall'], report_['micro avg']['f1-score'], pr, roc, 'Label Powerset-LR')\n",
    "\n",
    "    \n",
    "vals = [a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg]\n",
    "means = [np.mean(v) for v in vals]\n",
    "cis = [ci(v) for v in vals]\n",
    "record_values(*means, *cis, 'Label Powerset-LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg = [], [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(n):\n",
    "\n",
    "    classifier = LabelPowerset(RandomForestClassifier())\n",
    "    classifier.fit(train_features, train_labels)\n",
    "\n",
    "    predicted_labels_lp = classifier.predict_proba(test_features)\n",
    "    accuracy_, hamming_, brier_score_, report_ = evaluate(test_labels, predicted_labels_lp.toarray())\n",
    "    pr = plot_pr_curve(test_labels, predicted_labels_lp.toarray())\n",
    "    roc = plot_roc_curve(test_labels, predicted_labels_lp.toarray())\n",
    "    \n",
    "    a_avg.append(accuracy_)\n",
    "    h_avg.append(hamming_)\n",
    "    b_avg.append(brier_score_)\n",
    "    p_avg.append(report_['micro avg']['precision'])\n",
    "    r_avg.append(report_['micro avg']['recall'])\n",
    "    f_avg.append(report_['micro avg']['f1-score'])\n",
    "    pr_avg.append(pr)\n",
    "    roc_avg.append(roc)\n",
    "    record_values_all(accuracy_, hamming_, brier_score_, report_['micro avg']['precision'], report_['micro avg']['recall'], report_['micro avg']['f1-score'], pr, roc, 'Label Powerset-RFC')\n",
    "\n",
    "    \n",
    "vals = [a_avg, h_avg, b_avg, p_avg, r_avg, f_avg, pr_avg, roc_avg]\n",
    "means = [np.mean(v) for v in vals]\n",
    "cis = [ci(v) for v in vals]\n",
    "record_values(*means, *cis, 'Label Powerset-RFC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(a_avg), np.mean(h_avg), np.mean(b_avg), np.mean(p_avg), np.mean(r_avg), np.mean(f_avg), np.mean(pr_avg), np.mean(roc_avg),np.std(a_avg), np.std(h_avg), np.std(b_avg), np.std(p_avg), np.std(r_avg), np.std(f_avg), np.std(pr_avg), np.std(roc_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "score_names = ['accuracy_scores', 'hamming', 'brier_score', 'micro avg precision', 'micro avg recall', 'micro avg f1_score', 'micro pr_auc', 'micro roc_auc']\n",
    "for scores, scores_std, score_name in zip([accuracy_scores, hamming, brier_score, precision, recall, f1_score, pr_auc, roc_auc],[accuracy_scores_std, hamming_std, brier_score_std, precision_std, recall_std, f1_score_std, pr_auc_std, roc_auc_std], score_names):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    print(scores, scores_std)\n",
    "    scores_std = np.array(list(zip(*scores_std)))\n",
    "    scores_std[0] = scores - scores_std[0]\n",
    "    scores_std[1] = scores_std[1] - scores\n",
    "    ax.bar(model_names, scores, yerr = scores_std, capsize=10)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.title(score_name)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9205b650ad9b26ded1fb73bba57cf404cfc03cd0c8186ebe669ad7e6a2a6143"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
