{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import mat73\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = ''\n",
    "if 'google.colab' not in str(get_ipython()) and \"anuja\" in os.environ.get('USER'):\n",
    "    DATA_DIR = 'data/'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDs</th>\n",
       "      <th>Intercept</th>\n",
       "      <th>Slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDARAA075AMK</td>\n",
       "      <td>0.986272</td>\n",
       "      <td>1.825774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDARAA112DMH</td>\n",
       "      <td>1.486650</td>\n",
       "      <td>1.888544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDARAA117NEJ</td>\n",
       "      <td>1.593155</td>\n",
       "      <td>2.095749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDARAA947ZG5</td>\n",
       "      <td>0.703331</td>\n",
       "      <td>1.724831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDARAA948VFH</td>\n",
       "      <td>0.918020</td>\n",
       "      <td>1.749441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>NDARZN277NR6</td>\n",
       "      <td>1.351549</td>\n",
       "      <td>1.996940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>NDARZN578YDP</td>\n",
       "      <td>1.380795</td>\n",
       "      <td>2.036327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>NDARZN610GTY</td>\n",
       "      <td>0.339229</td>\n",
       "      <td>1.050644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>NDARZN677EYE</td>\n",
       "      <td>0.781225</td>\n",
       "      <td>1.470061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>NDARZN899JCM</td>\n",
       "      <td>0.464107</td>\n",
       "      <td>1.664433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2042 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               IDs  Intercept     Slope\n",
       "0     NDARAA075AMK   0.986272  1.825774\n",
       "1     NDARAA112DMH   1.486650  1.888544\n",
       "2     NDARAA117NEJ   1.593155  2.095749\n",
       "3     NDARAA947ZG5   0.703331  1.724831\n",
       "4     NDARAA948VFH   0.918020  1.749441\n",
       "...            ...        ...       ...\n",
       "2037  NDARZN277NR6   1.351549  1.996940\n",
       "2038  NDARZN578YDP   1.380795  2.036327\n",
       "2039  NDARZN610GTY   0.339229  1.050644\n",
       "2040  NDARZN677EYE   0.781225  1.470061\n",
       "2041  NDARZN899JCM   0.464107  1.664433\n",
       "\n",
       "[2042 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#here we use these columns nly to get the subject ID\n",
    "foof = pd.read_csv(DATA_DIR+\"foof2features.csv\")\n",
    "foof = foof.rename(columns={\"C1\": \"IDs\" ,\"C2\": \"Intercept\", \"C3\": \"Slope\"})\n",
    "foof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2041, 68, 391)\n"
     ]
    }
   ],
   "source": [
    "data = mat73.loadmat(DATA_DIR+'x_source.mat')  \n",
    "\n",
    "# flattening\n",
    "df2 = pd.DataFrame(data['x'].reshape((data['x'].shape[0], -1)))\n",
    "df2 = np.array(df2)\n",
    "df2 = df2.reshape((2041,68,391)) \n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2041, 68, 39)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsed= np.concatenate([np.expand_dims(df2[:,:,i:i+10].mean(axis = 2), axis = 2) for i in range(0,381,10)], axis = 2)\n",
    "\n",
    "#sanity check\n",
    "df2[0,0,10:20].mean()\n",
    "sparsed[0,0,1]\n",
    "sparsed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2041, 2652)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flattening and adding id\n",
    "df2 = pd.DataFrame(sparsed.reshape((sparsed.shape[0], -1)))\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2041, 2653)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['IDs'] = foof['IDs']\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:(3076, 177)\n",
      "After:(2813, 177)\n",
      "Removing 263 patients as their diagnoses were very uncommon.\n"
     ]
    }
   ],
   "source": [
    "beh = pd.read_csv(DATA_DIR+\"behaviorals.csv\")\n",
    "print('Before:'+str(beh.shape))\n",
    "\n",
    "most_common_disorders = ['Attention-Deficit/Hyperactivity Disorder', 'Anxiety Disorders', 'Specific Learning Disorder',\n",
    "                         'Autism Spectrum Disorder', 'Disruptive', 'No Diagnosis Given', 'Communication Disorder',\n",
    "                         'Depressive Disorders']\n",
    "\n",
    "# most_common_disorders = ['Other Neurodevelopmental Disorders', 'ADHD-Inattentive Type', 'ADHD-Combined Type', 'Anxiety Disorders', 'No Diagnosis Given', 'Depressive Disorders']\n",
    "\n",
    "category_columns = ['DX_' + str(i).zfill(2) + '_Cat' for i in range(1, 11)] +\\\n",
    "                   ['DX_' + str(i).zfill(2) + '_Sub' for i in range(1, 11)]\n",
    "\n",
    "# find users that have no diagnosis within these top diseases\n",
    "# filtering should cahnge anything as this should also happen at a later stage\n",
    "mask = None\n",
    "for col in category_columns:\n",
    "    mask_col = beh[col].isin(most_common_disorders)\n",
    "    if mask is None:\n",
    "        mask = mask_col\n",
    "    else:\n",
    "        mask = mask | mask_col\n",
    "\n",
    "initial_size = beh.shape[0]\n",
    "beh = beh[mask]\n",
    "beh = beh.reset_index(drop=True)\n",
    "new_size = beh.shape[0]\n",
    "print('After:'+str(beh.shape))\n",
    "print('Removing', initial_size - new_size,\n",
    "      'patients as their diagnoses were very uncommon.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Attention-Deficit/Hyperactivity Disorder': 0,\n",
       " 'Anxiety Disorders': 1,\n",
       " 'Specific Learning Disorder': 2,\n",
       " 'Autism Spectrum Disorder': 3,\n",
       " 'Disruptive': 4,\n",
       " 'Communication Disorder': 5,\n",
       " 'Depressive Disorders': 6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_diagnosis_given = 'No Diagnosis Given'\n",
    "\n",
    "if no_diagnosis_given in most_common_disorders:\n",
    "    no_diag_index = most_common_disorders.index(no_diagnosis_given)\n",
    "    most_common_disorders = most_common_disorders[:no_diag_index] + \\\n",
    "        most_common_disorders[no_diag_index + 1:]\n",
    "\n",
    "diagnoses_to_ids = {disorder: i for i, disorder in enumerate(most_common_disorders)}\n",
    "diagnoses_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disorder(data, row, index):\n",
    "    disorder = data.iloc[row][category_columns[index]]\n",
    "\n",
    "    if disorder == 'Neurodevelopmental Disorders':\n",
    "        disorder = data.iloc[row][category_columns[index + 10]]\n",
    "\n",
    "    return disorder\n",
    "\n",
    "order_of_disorders = []\n",
    "for k in range(beh.shape[0]):\n",
    "    i = 0\n",
    "    disorder = get_disorder(beh, k, i)\n",
    "    disorders_patient = []\n",
    "    while disorder != no_diagnosis_given and not pd.isnull(disorder):\n",
    "        if disorder in diagnoses_to_ids:\n",
    "            if diagnoses_to_ids[disorder] not in disorders_patient:\n",
    "                disorders_patient.append(diagnoses_to_ids[disorder])\n",
    "        i += 1\n",
    "        if i == len(category_columns):\n",
    "            break\n",
    "        disorder = get_disorder(beh, k, i)\n",
    "\n",
    "    order_of_disorders.append(disorders_patient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_len_order = np.max([len(x) for x in order_of_disorders])\n",
    "\n",
    "# pad with a new token denoting the pad token\n",
    "pad_token = len(most_common_disorders)\n",
    "bod_token = len(most_common_disorders) + 1\n",
    "eod_token = len(most_common_disorders) + 2\n",
    "\n",
    "order_of_disorders = [[bod_token] + x + [eod_token] + [pad_token] * (max_len_order - len(x)) for x in order_of_disorders]\n",
    "\n",
    "order_of_disorders = np.array(order_of_disorders)\n",
    "\n",
    "classes = np.zeros((len(most_common_disorders),\n",
    "                    beh.shape[0]), dtype=np.int32)\n",
    "\n",
    "df_disorders = beh[category_columns]\n",
    "\n",
    "for i, disorder in enumerate(most_common_disorders):\n",
    "    mask = df_disorders.select_dtypes(include=[object]). \\\n",
    "        applymap(lambda x: disorder in x if pd.notnull(x) else False)\n",
    "\n",
    "    disorder_df = df_disorders[mask.any(axis=1)]\n",
    "\n",
    "    np.add.at(classes[i], disorder_df.index.values, 1)\n",
    "\n",
    "behaviour_data_columns = beh.columns.values.astype(np.str)\n",
    "\n",
    "columns_to_drop = behaviour_data_columns[\n",
    "    np.flatnonzero(np.core.defchararray.find(behaviour_data_columns, 'DX') != -1)]\n",
    "\n",
    "behaviour_data = beh.drop(columns=columns_to_drop)\n",
    "\n",
    "for disorder, classification in zip(most_common_disorders, classes):\n",
    "    behaviour_data[disorder] = classification\n",
    "\n",
    "behaviour_data['order_diagnoses'] = list(order_of_disorders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDs</th>\n",
       "      <th>Attention-Deficit/Hyperactivity Disorder</th>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <th>Specific Learning Disorder</th>\n",
       "      <th>Autism Spectrum Disorder</th>\n",
       "      <th>Disruptive</th>\n",
       "      <th>Communication Disorder</th>\n",
       "      <th>Depressive Disorders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDARAA075AMK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDARAA112DMH</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDARAA117NEJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDARAA306NT2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDARAA504CRN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2808</th>\n",
       "      <td>NDARZZ007YMP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>NDARZZ740MLM</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>NDARZZ810LVF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>NDARZZ830JM7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>NDARZZ993CEV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2813 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               IDs  Attention-Deficit/Hyperactivity Disorder  \\\n",
       "0     NDARAA075AMK                                         0   \n",
       "1     NDARAA112DMH                                         1   \n",
       "2     NDARAA117NEJ                                         1   \n",
       "3     NDARAA306NT2                                         1   \n",
       "4     NDARAA504CRN                                         1   \n",
       "...            ...                                       ...   \n",
       "2808  NDARZZ007YMP                                         0   \n",
       "2809  NDARZZ740MLM                                         1   \n",
       "2810  NDARZZ810LVF                                         0   \n",
       "2811  NDARZZ830JM7                                         0   \n",
       "2812  NDARZZ993CEV                                         0   \n",
       "\n",
       "      Anxiety Disorders  Specific Learning Disorder  Autism Spectrum Disorder  \\\n",
       "0                     0                           0                         0   \n",
       "1                     0                           0                         0   \n",
       "2                     0                           0                         0   \n",
       "3                     1                           1                         0   \n",
       "4                     1                           1                         0   \n",
       "...                 ...                         ...                       ...   \n",
       "2808                  0                           0                         1   \n",
       "2809                  0                           0                         0   \n",
       "2810                  0                           0                         1   \n",
       "2811                  0                           0                         1   \n",
       "2812                  1                           0                         0   \n",
       "\n",
       "      Disruptive  Communication Disorder  Depressive Disorders  \n",
       "0              0                       0                     0  \n",
       "1              1                       0                     0  \n",
       "2              1                       0                     0  \n",
       "3              0                       1                     0  \n",
       "4              0                       0                     0  \n",
       "...          ...                     ...                   ...  \n",
       "2808           0                       0                     0  \n",
       "2809           0                       0                     0  \n",
       "2810           0                       1                     0  \n",
       "2811           0                       0                     0  \n",
       "2812           0                       0                     0  \n",
       "\n",
       "[2813 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=behaviour_data[[\"IDs\"]+list(most_common_disorders)]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835, 2660)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.merge(df2, labels, on='IDs', how='inner')\n",
    "df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835, 2660)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#removing NaNs\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1835, 2652) (1835, 7)\n"
     ]
    }
   ],
   "source": [
    "x = df[df.columns.difference(['IDs']+most_common_disorders)]\n",
    "y = df[most_common_disorders]\n",
    "\n",
    "# summarize dataset shape\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling features\n",
    "\n",
    "# data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(x)\n",
    "\n",
    "# transform training data\n",
    "x_norm = norm.transform(x)\n",
    "x_norm = x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(x, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate(y_test, y_pred):\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", skm.classification_report(y_test,y_pred, zero_division=1))\n",
    "    print(\"Confusion matrix:\\n\", skm.multilabel_confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Output Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression():\n",
      "Accuracy: 0.09437386569872959\n",
      "Hamming Loss: 0.2870106300233342\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.66      0.65       357\n",
      "           1       0.30      0.29      0.30       177\n",
      "           2       0.25      0.15      0.19       142\n",
      "           3       0.20      0.10      0.14        98\n",
      "           4       0.17      0.07      0.10        82\n",
      "           5       0.09      0.03      0.05        88\n",
      "           6       0.04      0.02      0.03        51\n",
      "\n",
      "   micro avg       0.43      0.33      0.37       995\n",
      "   macro avg       0.24      0.19      0.21       995\n",
      "weighted avg       0.36      0.33      0.34       995\n",
      " samples avg       0.54      0.43      0.34       995\n",
      "\n",
      "Confusion matrix:\n",
      " [[[ 61 133]\n",
      "  [120 237]]\n",
      "\n",
      " [[252 122]\n",
      "  [125  52]]\n",
      "\n",
      " [[345  64]\n",
      "  [121  21]]\n",
      "\n",
      " [[414  39]\n",
      "  [ 88  10]]\n",
      "\n",
      " [[439  30]\n",
      "  [ 76   6]]\n",
      "\n",
      " [[431  32]\n",
      "  [ 85   3]]\n",
      "\n",
      " [[478  22]\n",
      "  [ 50   1]]]\n",
      "RandomForestClassifier(random_state=1):\n",
      "Accuracy: 0.16152450090744103\n",
      "Hamming Loss: 0.2172673061965258\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.96      0.78       357\n",
      "           1       0.40      0.03      0.06       177\n",
      "           2       0.00      0.00      0.00       142\n",
      "           3       0.67      0.02      0.04        98\n",
      "           4       0.00      0.00      0.00        82\n",
      "           5       0.75      0.03      0.07        88\n",
      "           6       0.00      0.00      0.00        51\n",
      "\n",
      "   micro avg       0.64      0.35      0.46       995\n",
      "   macro avg       0.35      0.15      0.14       995\n",
      "weighted avg       0.44      0.35      0.30       995\n",
      " samples avg       0.67      0.46      0.44       995\n",
      "\n",
      "Confusion matrix:\n",
      " [[[ 16 178]\n",
      "  [ 15 342]]\n",
      "\n",
      " [[365   9]\n",
      "  [171   6]]\n",
      "\n",
      " [[407   2]\n",
      "  [142   0]]\n",
      "\n",
      " [[452   1]\n",
      "  [ 96   2]]\n",
      "\n",
      " [[466   3]\n",
      "  [ 82   0]]\n",
      "\n",
      " [[462   1]\n",
      "  [ 85   3]]\n",
      "\n",
      " [[498   2]\n",
      "  [ 51   0]]]\n",
      "SVC():\n",
      "Accuracy: 0.15245009074410162\n",
      "Hamming Loss: 0.21571169302566762\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79       357\n",
      "           1       1.00      0.00      0.00       177\n",
      "           2       1.00      0.00      0.00       142\n",
      "           3       1.00      0.00      0.00        98\n",
      "           4       1.00      0.00      0.00        82\n",
      "           5       1.00      0.00      0.00        88\n",
      "           6       1.00      0.00      0.00        51\n",
      "\n",
      "   micro avg       0.65      0.36      0.46       995\n",
      "   macro avg       0.95      0.14      0.11       995\n",
      "weighted avg       0.87      0.36      0.28       995\n",
      " samples avg       0.65      0.47      0.44       995\n",
      "\n",
      "Confusion matrix:\n",
      " [[[  0 194]\n",
      "  [  0 357]]\n",
      "\n",
      " [[374   0]\n",
      "  [177   0]]\n",
      "\n",
      " [[409   0]\n",
      "  [142   0]]\n",
      "\n",
      " [[453   0]\n",
      "  [ 98   0]]\n",
      "\n",
      " [[469   0]\n",
      "  [ 82   0]]\n",
      "\n",
      " [[463   0]\n",
      "  [ 88   0]]\n",
      "\n",
      " [[500   0]\n",
      "  [ 51   0]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC as svm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "forest = RandomForestClassifier(random_state=1)\n",
    "lg = LogisticRegression()\n",
    "svm = svm()\n",
    "models = [lg, forest, svm]\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    multi_output_model = MultiOutputClassifier(model, n_jobs=-1)\n",
    "    multi_output_model.fit(train_features, train_labels)\n",
    "    predicted_labels = multi_output_model.predict(test_features)\n",
    "    print(str(model)+':')\n",
    "    evaluate(test_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 13:03:40.192619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-20 13:03:40.221640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-20 13:03:40.222306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:06.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-11-20 13:03:40.222512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-20 13:03:40.223702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-20 13:03:40.225035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-20 13:03:40.225220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-20 13:03:40.226439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-20 13:03:40.227337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-20 13:03:40.230009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-20 13:03:40.230165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-20 13:03:40.230885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-20 13:03:40.231496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2021-11-20 13:03:40.231955: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2021-11-20 13:03:40.237819: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1996245000 Hz\n",
      "2021-11-20 13:03:40.238297: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a0b6935f60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-20 13:03:40.238306: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-11-20 13:03:40.238485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-20 13:03:40.239126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:06.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-11-20 13:03:40.239167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-20 13:03:40.239181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-20 13:03:40.239192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-20 13:03:40.239204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-20 13:03:40.239214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-20 13:03:40.239226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-20 13:03:40.239237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-20 13:03:40.239296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-20 13:03:40.239948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-20 13:03:40.240548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2021-11-20 13:03:40.240583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-20 13:03:40.313911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-20 13:03:40.313939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2021-11-20 13:03:40.313944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2021-11-20 13:03:40.314371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-20 13:03:40.315038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-20 13:03:40.315654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-20 13:03:40.316248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14109 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5)\n",
      "2021-11-20 13:03:40.317941: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a0bc9e27e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-20 13:03:40.317952: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 13:03:41.243512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 1ms/step - loss: 0.5163\n",
      "Epoch 2/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.5038\n",
      "Epoch 3/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4933\n",
      "Epoch 4/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4938\n",
      "Epoch 5/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4920\n",
      "Epoch 6/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4815\n",
      "Epoch 7/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4792\n",
      "Epoch 8/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4821\n",
      "Epoch 9/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4742\n",
      "Epoch 10/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4681\n",
      "Epoch 11/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4694\n",
      "Epoch 12/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4629\n",
      "Epoch 13/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4600\n",
      "Epoch 14/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4555\n",
      "Epoch 15/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4570\n",
      "Epoch 16/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4508\n",
      "Epoch 17/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4546\n",
      "Epoch 18/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4516\n",
      "Epoch 19/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4452\n",
      "Epoch 20/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4380\n",
      "Epoch 21/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4390\n",
      "Epoch 22/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4324\n",
      "Epoch 23/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4336\n",
      "Epoch 24/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4275\n",
      "Epoch 25/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4337\n",
      "Epoch 26/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4287\n",
      "Epoch 27/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4237\n",
      "Epoch 28/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4307\n",
      "Epoch 29/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.4114\n",
      "Epoch 30/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4146\n",
      "Epoch 31/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4108\n",
      "Epoch 32/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4065\n",
      "Epoch 33/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3991\n",
      "Epoch 34/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4025\n",
      "Epoch 35/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3918\n",
      "Epoch 36/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3932\n",
      "Epoch 37/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.4011\n",
      "Epoch 38/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3858\n",
      "Epoch 39/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3888\n",
      "Epoch 40/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3762\n",
      "Epoch 41/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3848\n",
      "Epoch 42/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3752\n",
      "Epoch 43/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3667\n",
      "Epoch 44/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3678\n",
      "Epoch 45/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3628\n",
      "Epoch 46/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3669\n",
      "Epoch 47/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3614\n",
      "Epoch 48/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3684\n",
      "Epoch 49/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3609\n",
      "Epoch 50/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3524\n",
      "Epoch 51/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3508\n",
      "Epoch 52/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3435\n",
      "Epoch 53/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3445\n",
      "Epoch 54/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3391\n",
      "Epoch 55/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3435\n",
      "Epoch 56/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3328\n",
      "Epoch 57/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3380\n",
      "Epoch 58/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3306\n",
      "Epoch 59/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3235\n",
      "Epoch 60/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3289\n",
      "Epoch 61/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3199\n",
      "Epoch 62/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3162\n",
      "Epoch 63/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3132\n",
      "Epoch 64/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3159\n",
      "Epoch 65/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3085\n",
      "Epoch 66/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3066\n",
      "Epoch 67/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3073\n",
      "Epoch 68/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2985\n",
      "Epoch 69/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3002\n",
      "Epoch 70/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2981\n",
      "Epoch 71/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3023\n",
      "Epoch 72/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2888\n",
      "Epoch 73/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2940\n",
      "Epoch 74/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3017\n",
      "Epoch 75/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2811\n",
      "Epoch 76/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2781\n",
      "Epoch 77/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2767\n",
      "Epoch 78/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2797\n",
      "Epoch 79/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2770\n",
      "Epoch 80/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2727\n",
      "Epoch 81/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2685\n",
      "Epoch 82/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2624\n",
      "Epoch 83/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2666\n",
      "Epoch 84/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2620\n",
      "Epoch 85/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2611\n",
      "Epoch 86/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2585\n",
      "Epoch 87/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2582\n",
      "Epoch 88/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2570\n",
      "Epoch 89/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2513\n",
      "Epoch 90/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2543\n",
      "Epoch 91/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2466\n",
      "Epoch 92/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2414\n",
      "Epoch 93/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2509\n",
      "Epoch 94/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2512\n",
      "Epoch 95/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2397\n",
      "Epoch 96/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2353\n",
      "Epoch 97/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2309\n",
      "Epoch 98/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2369\n",
      "Epoch 99/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2365\n",
      "Epoch 100/100\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f45aca6b6a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def get_mlp(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "n_inputs, n_outputs = x.shape[1], y.shape[1]\n",
    "mlp = get_mlp(n_inputs, n_outputs)\n",
    "mlp.fit(train_features, train_labels, verbose=1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.11796733212341198\n",
      "Hamming Loss: 0.27223230490018147\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.45      0.53       357\n",
      "           1       0.31      0.32      0.32       177\n",
      "           2       0.30      0.11      0.16       142\n",
      "           3       0.50      0.03      0.06        98\n",
      "           4       0.24      0.05      0.08        82\n",
      "           5       0.11      0.01      0.02        88\n",
      "           6       0.08      0.04      0.05        51\n",
      "\n",
      "   micro avg       0.45      0.24      0.32       995\n",
      "   macro avg       0.31      0.14      0.17       995\n",
      "weighted avg       0.41      0.24      0.29       995\n",
      " samples avg       0.64      0.34      0.29       995\n",
      "\n",
      "Confusion matrix:\n",
      " [[[103  91]\n",
      "  [197 160]]\n",
      "\n",
      " [[252 122]\n",
      "  [121  56]]\n",
      "\n",
      " [[372  37]\n",
      "  [126  16]]\n",
      "\n",
      " [[450   3]\n",
      "  [ 95   3]]\n",
      "\n",
      " [[456  13]\n",
      "  [ 78   4]]\n",
      "\n",
      " [[455   8]\n",
      "  [ 87   1]]\n",
      "\n",
      " [[477  23]\n",
      "  [ 49   2]]]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_mlp = mlp.predict(test_features)\n",
    "evaluate(test_labels, predicted_labels_mlp.round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Relevance\n",
    "ignores the possible correlations between class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=GaussianNB(), require_dense=[True, True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "classifier.fit(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16152450090744103\n",
      "Hamming Loss: 0.23463831993777548\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87       357\n",
      "           1       0.63      0.82      0.72       177\n",
      "           2       0.44      0.72      0.54       142\n",
      "           3       0.44      0.66      0.53        98\n",
      "           4       0.28      0.77      0.41        82\n",
      "           5       0.38      0.73      0.50        88\n",
      "           6       0.32      0.86      0.46        51\n",
      "\n",
      "   micro avg       0.53      0.78      0.63       995\n",
      "   macro avg       0.48      0.77      0.58       995\n",
      "weighted avg       0.62      0.78      0.67       995\n",
      " samples avg       0.54      0.82      0.59       995\n",
      "\n",
      "Confusion matrix:\n",
      " [[[165  29]\n",
      "  [ 61 296]]\n",
      "\n",
      " [[289  85]\n",
      "  [ 31 146]]\n",
      "\n",
      " [[277 132]\n",
      "  [ 40 102]]\n",
      "\n",
      " [[369  84]\n",
      "  [ 33  65]]\n",
      "\n",
      " [[308 161]\n",
      "  [ 19  63]]\n",
      "\n",
      " [[359 104]\n",
      "  [ 24  64]]\n",
      "\n",
      " [[405  95]\n",
      "  [  7  44]]]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_br = classifier.predict(test_features)\n",
    "evaluate(test_labels, predicted_labels_br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classfier Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(classifier=LogisticRegression(), require_dense=[True, True])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = ClassifierChain(LogisticRegression())\n",
    "classifier.fit(train_features, train_labels)\n",
    "# we should optimise this a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.08892921960072596\n",
      "Hamming Loss: 0.2859735545760954\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.66      0.65       357\n",
      "           1       0.30      0.29      0.29       177\n",
      "           2       0.25      0.14      0.18       142\n",
      "           3       0.22      0.12      0.16        98\n",
      "           4       0.23      0.11      0.15        82\n",
      "           5       0.10      0.05      0.06        88\n",
      "           6       0.12      0.06      0.08        51\n",
      "\n",
      "   micro avg       0.43      0.34      0.38       995\n",
      "   macro avg       0.27      0.20      0.22       995\n",
      "weighted avg       0.37      0.34      0.35       995\n",
      " samples avg       0.55      0.44      0.34       995\n",
      "\n",
      "Confusion matrix:\n",
      " [[[ 61 133]\n",
      "  [120 237]]\n",
      "\n",
      " [[253 121]\n",
      "  [126  51]]\n",
      "\n",
      " [[348  61]\n",
      "  [122  20]]\n",
      "\n",
      " [[411  42]\n",
      "  [ 86  12]]\n",
      "\n",
      " [[438  31]\n",
      "  [ 73   9]]\n",
      "\n",
      " [[428  35]\n",
      "  [ 84   4]]\n",
      "\n",
      " [[479  21]\n",
      "  [ 48   3]]]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_cc = classifier.predict(test_features)\n",
    "evaluate(test_labels, predicted_labels_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Powerset\n",
    "takes correlations into account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelPowerset(classifier=LogisticRegression(), require_dense=[True, True])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "classifier = LabelPowerset(LogisticRegression())\n",
    "classifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10526315789473684\n",
      "Hamming Loss: 0.26186155042779363\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.65      0.64       357\n",
      "           1       0.36      0.23      0.28       177\n",
      "           2       0.36      0.15      0.21       142\n",
      "           3       0.28      0.13      0.18        98\n",
      "           4       0.18      0.09      0.12        82\n",
      "           5       0.11      0.01      0.02        88\n",
      "           6       0.17      0.06      0.09        51\n",
      "\n",
      "   micro avg       0.49      0.32      0.38       995\n",
      "   macro avg       0.30      0.19      0.22       995\n",
      "weighted avg       0.40      0.32      0.34       995\n",
      " samples avg       0.62      0.41      0.35       995\n",
      "\n",
      "Confusion matrix:\n",
      " [[[ 61 133]\n",
      "  [126 231]]\n",
      "\n",
      " [[302  72]\n",
      "  [137  40]]\n",
      "\n",
      " [[372  37]\n",
      "  [121  21]]\n",
      "\n",
      " [[419  34]\n",
      "  [ 85  13]]\n",
      "\n",
      " [[437  32]\n",
      "  [ 75   7]]\n",
      "\n",
      " [[455   8]\n",
      "  [ 87   1]]\n",
      "\n",
      " [[485  15]\n",
      "  [ 48   3]]]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_lp = classifier.predict(test_features)\n",
    "evaluate(test_labels, predicted_labels_lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Label KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "mlknn = MLkNN(k=10)\n",
    "\n",
    "x_train = lil_matrix(train_features).toarray()\n",
    "y_train = lil_matrix(train_labels).toarray()\n",
    "x_test = lil_matrix(test_features).toarray()\n",
    "# train\n",
    "mlknn.fit(x_train, y_train)\n",
    "# predict\n",
    "# predictions_new = classifier_new.predict(x_test)\n",
    "# # accuracy\n",
    "# print(\"Accuracy = \",accuracy_score(y_test,predictions_new))\n",
    "# print(\"\\n\")\n",
    "predicted_labels_mlknn = mlknn.predict(x_test)\n",
    "evaluate(test_labels, predicted_labels_mlknn)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9205b650ad9b26ded1fb73bba57cf404cfc03cd0c8186ebe669ad7e6a2a6143"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
