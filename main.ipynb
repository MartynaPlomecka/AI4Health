{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartynaPlomecka/AI4Health/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "23TOba33L4qf",
        "outputId": "0fe3617d-69a4-441e-9624-5f0fc14eb95a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Apr 17 15:28:22 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    27W /  70W |   8466MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1G82GuO-tez",
        "outputId": "d5ca9037-fcf7-462c-c57e-3f9238850aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXwgOm4wgULj",
        "outputId": "a7562cb7-6c62-490a-9d54-164df7553b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "## for data\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics, manifold\n",
        "## for processing\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "## for bert\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import sys\n",
        "import gzip\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd_ONue3gtFy"
      },
      "outputs": [],
      "source": [
        "# mount to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# dataset paths\n",
        "dataset_path = '/content/drive/My Drive/embeddings/table4model_clean.csv'\n",
        "not_labelled_indeed_path = '/content/drive/My Drive/embeddings/indeed_intro_sh.csv'\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dtf = pd.read_csv(not_labelled_indeed_path, header = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0QWaYODhEm6"
      },
      "outputs": [],
      "source": [
        "#df2.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADukPNBOhmv2"
      },
      "outputs": [],
      "source": [
        "## rename columns\n",
        "dtf = dtf.rename(columns={ \"Introduction\":\"text\"})\n",
        "## print 5 random rows\n",
        "dtf.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_YhNSOyhywQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Preprocess indeed database BEFORE MLM\n",
        ":parameter\n",
        "    :param text: string - name of column containing text\n",
        "    :param lst_stopwords: list - list of stopwords to remove\n",
        "    :param flg_stemm: bool - whether stemming is to be applied\n",
        "    :param flg_lemm: bool - whether lemmitisation is to be applied\n",
        ":return\n",
        "    cleaned df for  masked language modeling (MLM)\n",
        "'''\n",
        "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
        "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
        "            \n",
        "    ## Tokenize (convert from string to list)\n",
        "    lst_text = text.split()    \n",
        "    ## remove Stopwords\n",
        "    if lst_stopwords is not None:\n",
        "        lst_text = [word for word in lst_text if word not in \n",
        "                    lst_stopwords]\n",
        "                \n",
        "    ## Stemming (remove -ing, -ly, ...)\n",
        "    if flg_stemm == True:\n",
        "        ps = nltk.stem.porter.PorterStemmer()\n",
        "        lst_text = [ps.stem(word) for word in lst_text]\n",
        "                \n",
        "    ## Lemmatisation (convert the word into root word)\n",
        "    if flg_lemm == True:\n",
        "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
        "            \n",
        "    ## back to string from list\n",
        "    text = \" \".join(lst_text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLx4w0pWiDIq"
      },
      "outputs": [],
      "source": [
        "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "lst_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M30LY33-iP-S"
      },
      "outputs": [],
      "source": [
        "dtf[\"text_clean\"] = dtf[\"text\"].apply(lambda x: \n",
        "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
        "          lst_stopwords=lst_stopwords))\n",
        "dtf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpzrLvy0jsPV"
      },
      "outputs": [],
      "source": [
        "df = dtf[['text_clean']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m9PgsXGmdcd"
      },
      "outputs": [],
      "source": [
        "df.to_csv('data.csv', index = False)\n",
        "!cp data.csv \"drive/My Drive/embeddings/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2nMGzuuD0GA"
      },
      "source": [
        "# masked language modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Masked Language Model (MLM) is the process how BERT was pre-trained. MLM is a powerful pre-training strategy for learning sentence embeddings. This is especially the case when we work on a specialized domain.\n",
        "\n"
      ],
      "metadata": {
        "id": "2B20fgveKwBa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elKR3dZkEj9I"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import sys\n",
        "import gzip\n",
        "from datetime import datetime\n",
        "no_deprecation_warning=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWQ69aIXEpZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8bd3bb5-3ab7-43e5-b31e-5d899a9131af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
            "\n",
            "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "model_name = sys.argv[1]\n",
        "per_device_train_batch_size = 64\n",
        "\n",
        "save_steps = 1000               #Save model every 1k steps\n",
        "num_train_epochs = 3            #Number of epochs\n",
        "use_fp16 = False                #Set to True, if your GPU supports FP16 operations\n",
        "max_length = 100                #Max length for a text input\n",
        "do_whole_word_mask = True       #If set to true, whole words are masked\n",
        "mlm_prob = 0.15                 #Probability that a word is replaced by a [MASK] token\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXgFZjVVE-ol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d7376f-bbcc-4f2d-d928-6eab15f27b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoints to: output/-f-2022-04-17_15-32-27\n"
          ]
        }
      ],
      "source": [
        "\n",
        "output_dir = \"output/{}-{}\".format(model_name.replace(\"/\", \"_\"),  datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "print(\"Save checkpoints to:\", output_dir)\n",
        "\n",
        "no_deprecation_warning=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaIBQmDcD0o-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "52636e3a-44d1-4582-dab9-b8221476d6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "tokenizer config file saved in output/-f-2022-04-17_15-32-27/tokenizer_config.json\n",
            "Special tokens file saved in output/-f-2022-04-17_15-32-27/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3818\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sentences: 3818\n",
            "Dev sentences: 0\n",
            "Save tokenizer to: output/-f-2022-04-17_15-32-27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [180/180 02:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Configuration saved in output/-f-2022-04-17_15-32-27/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save model to: output/-f-2022-04-17_15-32-27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in output/-f-2022-04-17_15-32-27/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training done\n"
          ]
        }
      ],
      "source": [
        "##### Load train dataset\n",
        "\n",
        "train_sentences = []\n",
        "train_path = '/content/drive/My Drive/embeddings/dev.txt'\n",
        "with gzip.open(train_path, 'rt', encoding='utf8') if train_path.endswith('.gz') else  open(train_path, 'r', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        line = line.strip()\n",
        "        if len(line) >= 10:\n",
        "            train_sentences.append(line)\n",
        "\n",
        "print(\"Train sentences:\", len(train_sentences))\n",
        "\n",
        "dev_sentences = []\n",
        "if len(sys.argv) >= 4:\n",
        "    dev_path = sys.argv[3]\n",
        "    with gzip.open(dev_path, 'rt', encoding='utf8') if dev_path.endswith('.gz') else open(dev_path, 'r', encoding='utf8') as fIn:\n",
        "        for line in fIn:\n",
        "            line = line.strip()\n",
        "            if len(line) >= 10:\n",
        "                dev_sentences.append(line)\n",
        "\n",
        "print(\"Dev sentences:\", len(dev_sentences))\n",
        "\n",
        "#A dataset wrapper, that tokenizes our data on-the-fly\n",
        "class TokenizedSentencesDataset:\n",
        "    def __init__(self, sentences, tokenizer, max_length, cache_tokenization=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sentences = sentences\n",
        "        self.max_length = max_length\n",
        "        self.cache_tokenization = cache_tokenization\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if not self.cache_tokenization:\n",
        "            return self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
        "\n",
        "        if isinstance(self.sentences[item], str):\n",
        "            self.sentences[item] = self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
        "        return self.sentences[item]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "train_dataset = TokenizedSentencesDataset(train_sentences, tokenizer, max_length)\n",
        "dev_dataset = TokenizedSentencesDataset(dev_sentences, tokenizer, max_length, cache_tokenization=True) if len(dev_sentences) > 0 else None\n",
        "\n",
        "##### Training arguments\n",
        "\n",
        "if do_whole_word_mask:\n",
        "    data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
        "else:\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    evaluation_strategy=\"steps\" if dev_dataset is not None else \"no\",\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    eval_steps=save_steps,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=save_steps,\n",
        "    save_total_limit=1,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=use_fp16\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset\n",
        ")\n",
        "\n",
        "print(\"Save tokenizer to:\", output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Save model to:\", output_dir)\n",
        "model.save_pretrained(output_dir)\n",
        "\n",
        "print(\"Training done\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "main",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}