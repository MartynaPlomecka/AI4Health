{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mat73\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.svm import SVC as svm\n",
    "from sklearn.linear_model import LogisticRegression as lg\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,balanced_accuracy_score,f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from warnings import simplefilter,filterwarnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "DATA_DIR = ''\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    if \"anuja\" in os.environ.get('USER'):\n",
    "        DATA_DIR = 'data/'\n",
    "    elif 'ubuntu' in os.environ.get('USER'):\n",
    "        DATA_DIR = '/home/ubuntu/Martyna/repo/AI4Health/DATAfoof/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDs</th>\n",
       "      <th>WISC_WMI_Sum</th>\n",
       "      <th>WISC_VCI_Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WISC1_001</td>\n",
       "      <td>WISC1_039</td>\n",
       "      <td>WISC1_033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDARNL599TMZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDARJF045MHG</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDARZE542ZVH</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDARYH480GTD</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>NDARJG086ZMJ</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>NDARNC148NJ1</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>NDARWW744VLD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>NDARWE582CAV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>NDARBE680TCD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2938 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               IDs WISC_WMI_Sum WISC_VCI_Sum\n",
       "0        WISC1_001    WISC1_039    WISC1_033\n",
       "1     NDARNL599TMZ          NaN          NaN\n",
       "2     NDARJF045MHG           17           18\n",
       "3     NDARZE542ZVH           10           21\n",
       "4     NDARYH480GTD           16           20\n",
       "...            ...          ...          ...\n",
       "2933  NDARJG086ZMJ           24           28\n",
       "2934  NDARNC148NJ1           29           20\n",
       "2935  NDARWW744VLD          NaN          NaN\n",
       "2936  NDARWE582CAV          NaN          NaN\n",
       "2937  NDARBE680TCD          NaN          NaN\n",
       "\n",
       "[2938 rows x 3 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = scipy.io.loadmat(DATA_DIR+'x.mat')  \n",
    "df = pd.DataFrame(data['x'].reshape((data['x'].shape[0], -1)))\n",
    "foof = pd.read_csv(DATA_DIR+\"sensor_intercept_slope.csv\")\n",
    "\n",
    "# sparsing\n",
    "df = np.array(df).reshape(data['x'].shape)\n",
    "df_sparsed = np.concatenate([np.expand_dims(df[:,:,i:i+2].mean(axis = 2), axis = 2) for i in range(0, data['x'].shape[2]-2, 2)], axis = 2)\n",
    "df = pd.DataFrame(df_sparsed.reshape((df_sparsed.shape[0], -1)))\n",
    "\n",
    "\n",
    "# #scaling\n",
    "norm = MinMaxScaler().fit(df)\n",
    "df = norm.transform(df)\n",
    "df = pd.DataFrame(df.reshape((df.shape[0], -1)))\n",
    "\n",
    "\n",
    "\n",
    "columns = np.asarray([['Electrode %d - %d/2 Hz'%(i+1, j+1)] for i in range(df_sparsed.shape[1]) for j in range(df_sparsed.shape[2])])\n",
    "df.columns = columns\n",
    "df['IDs'] = foof['IDs']\n",
    "\n",
    "\n",
    "\n",
    "wisc = pd.read_csv(DATA_DIR+\"9994_WISC_20211209.csv\")\n",
    "wisc = wisc.rename(columns={\"EID\": \"IDs\"})\n",
    "wisc = wisc[['IDs', 'WISC_WMI_Sum','WISC_VCI_Sum']]\n",
    "wisc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1627, 4308)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, foof, on='IDs', how='inner')\n",
    "df = pd.merge(df, wisc, on='IDs', how='inner')\n",
    "\n",
    "\n",
    "#removing NaNs\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1627, 2)\n",
      "(1627, 4305)\n"
     ]
    }
   ],
   "source": [
    "#labels_list = ['WISC_WMI_Sum']\n",
    "labels_list = ['WISC_WMI_Sum','WISC_VCI_Sum']\n",
    "\n",
    "Y = df[labels_list]\n",
    "print(Y.shape)\n",
    "X = df[df.columns.difference(['IDs']+labels_list)]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f2713cd5e80>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(df, x='WISC_WMI_Sum', binwidth=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import power_transform\n",
    "Y = power_transform(Y, method='box-cox')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1627, 2)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 402.375x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(Y, binwidth=0.05)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/utils/validation.py:1675: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/utils/validation.py:1675: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/ai4halth/lib/python3.8/site-packages/sklearn/utils/validation.py:1675: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1220, 4305) (407, 4305)\n",
      "Applying PCA...\n",
      "(1220, 100) (407, 100)\n"
     ]
    }
   ],
   "source": [
    "# scaling x\n",
    "norm = preprocessing.MinMaxScaler().fit(xtrain)\n",
    "\n",
    "# transform training data\n",
    "xtrain = norm.transform(xtrain)\n",
    "xtest = norm.transform(xtest)\n",
    "print(xtrain.shape, xtest.shape)\n",
    "\n",
    "print('Applying PCA...')\n",
    "pca = PCA(.90) # 95% variance retained\n",
    "pca.fit(xtrain)\n",
    "\n",
    "# transform data\n",
    "xtrain = pca.transform(xtrain)\n",
    "xtest = pca.transform(xtest)\n",
    "print(xtrain.shape, xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1220, 2) (407, 2)\n"
     ]
    }
   ],
   "source": [
    "# scaling y, in case they have different values\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(ytrain)\n",
    "ytrain = min_max_scaler.transform(ytrain)\n",
    "ytest = min_max_scaler.transform(ytest)\n",
    "\n",
    "print(ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (dummy): 0.03\n",
      "Median absolute error (dummy): 0.10\n",
      "r2_score (dummy mean): -0.00\n",
      "r2_score (dummy median): -0.00\n",
      "[[0.30768774 0.53958031]\n",
      " [0.54513169 0.65576888]\n",
      " [0.48769366 0.68468743]\n",
      " [0.45854775 0.51039283]\n",
      " [0.39930516 0.45182983]\n",
      " [0.36916182 0.59778186]\n",
      " [0.71165815 0.51039283]\n",
      " [0.         0.0622873 ]\n",
      " [0.68443747 0.7135586 ]\n",
      " [0.57345644 0.42244751]]\n",
      "-3.7557882775272657e-16\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, median_absolute_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "\n",
    "lm_dummy_mean = DummyRegressor(strategy = 'mean').fit(xtrain, ytrain)\n",
    "lm_dummy_median = DummyRegressor(strategy = 'median').fit(xtrain, ytrain)\n",
    "ypred_dummy_mean = lm_dummy_mean.predict(xtest)\n",
    "ypred_dummy_median = lm_dummy_median.predict(xtest)\n",
    "\n",
    "\n",
    "print(\"Mean squared error (dummy): {:.2f}\".format(mean_squared_error(ytest,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\typred_dummy_mean)))\n",
    "\n",
    "print(\"Median absolute error (dummy): {:.2f}\".format(median_absolute_error(ytest,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\typred_dummy_median)))\n",
    "\n",
    "print(\"r2_score (dummy mean): {:.2f}\".format(r2_score(ytest, ypred_dummy_mean)))\n",
    "print(\"r2_score (dummy median): {:.2f}\".format(r2_score(ytest, ypred_dummy_median)))\n",
    "\n",
    "print(ytest[:10])\n",
    "print(Y.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiOutputReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (SVC): 0.03\n",
      "R2 score (SVC): -0.04\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Create the SVR regressor\n",
    "svr = SVR(epsilon=0.2)\n",
    "#Create the Multioutput Regressor\n",
    "mor = MultiOutputRegressor(svr)\n",
    "# Train the regressor\n",
    "mor = mor.fit(xtrain, ytrain)\n",
    "# Generate predictions for testing data\n",
    "ypred = mor.predict(xtest)\n",
    "\n",
    "\n",
    "print(\"Mean squared error (SVC): {:.2f}\".format(mean_squared_error(ytest,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\typred)))\n",
    "print(\"R2 score (SVC): {:.2f}\".format(mor.score(xtest, ytest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOR RanFor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (regr_multirf): 0.03\n",
      "R2 score (regr_multirf): 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "max_depth = 30\n",
    "regr_multirf = MultiOutputRegressor(\n",
    "    RandomForestRegressor(n_estimators=100, max_depth=max_depth, random_state=7)\n",
    ")\n",
    "regr_multirf.fit(xtrain, ytrain)\n",
    "y_multirf = regr_multirf.predict(xtest)\n",
    "\n",
    "\n",
    "print(\"Mean squared error (regr_multirf): {:.2f}\".format(mean_squared_error(ytest,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\typred)))\n",
    "print(\"R2 score (regr_multirf): {:.2f}\".format(regr_multirf.score(xtest, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense_15 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "488/488 [==============================] - 1s 2ms/step - loss: 0.1012 - mse: 0.1012 - val_loss: 0.0458 - val_mse: 0.0458\n",
      "Epoch 2/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 3/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 4/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 5/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 6/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 7/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 8/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 9/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 10/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 11/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 12/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 13/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 14/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 15/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 16/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 17/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 18/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 19/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0378 - val_mse: 0.0378\n",
      "Epoch 20/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0382 - val_mse: 0.0382\n",
      "Epoch 21/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 22/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 23/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 24/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 25/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 26/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0399 - val_mse: 0.0399\n",
      "Epoch 27/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 28/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0397 - val_mse: 0.0397\n",
      "Epoch 29/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0424 - val_mse: 0.0424\n",
      "Epoch 30/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0408 - val_mse: 0.0408\n",
      "Epoch 31/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0405 - val_mse: 0.0405\n",
      "Epoch 32/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0412 - val_mse: 0.0412\n",
      "Epoch 33/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0400 - val_mse: 0.0400\n",
      "Epoch 34/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0414 - val_mse: 0.0414\n",
      "Epoch 35/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0419 - val_mse: 0.0419\n",
      "Epoch 36/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0431 - val_mse: 0.0431\n",
      "Epoch 37/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0427 - val_mse: 0.0427\n",
      "Epoch 38/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0427 - val_mse: 0.0427\n",
      "Epoch 39/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0435 - val_mse: 0.0435\n",
      "Epoch 40/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0436 - val_mse: 0.0436\n",
      "Epoch 41/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0422 - val_mse: 0.0422\n",
      "Epoch 42/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0434 - val_mse: 0.0434\n",
      "Epoch 43/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0434 - val_mse: 0.0434\n",
      "Epoch 44/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0436 - val_mse: 0.0436\n",
      "Epoch 45/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0434 - val_mse: 0.0434\n",
      "Epoch 46/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0432 - val_mse: 0.0432\n",
      "Epoch 47/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0458 - val_mse: 0.0458\n",
      "Epoch 48/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0442 - val_mse: 0.0442\n",
      "Epoch 49/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0450 - val_mse: 0.0450\n",
      "Epoch 50/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0438 - val_mse: 0.0438\n",
      "Epoch 51/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0432 - val_mse: 0.0432\n",
      "Epoch 52/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0447 - val_mse: 0.0447\n",
      "Epoch 53/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0456 - val_mse: 0.0456\n",
      "Epoch 54/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0457 - val_mse: 0.0457\n",
      "Epoch 55/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0466 - val_mse: 0.0466\n",
      "Epoch 56/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0480 - val_mse: 0.0480\n",
      "Epoch 57/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0470 - val_mse: 0.0470\n",
      "Epoch 58/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0472 - val_mse: 0.0472\n",
      "Epoch 59/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0465 - val_mse: 0.0465\n",
      "Epoch 60/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0471 - val_mse: 0.0471\n",
      "Epoch 61/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0461 - val_mse: 0.0461\n",
      "Epoch 62/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0486 - val_mse: 0.0486\n",
      "Epoch 63/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0471 - val_mse: 0.0471\n",
      "Epoch 64/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0473 - val_mse: 0.0473\n",
      "Epoch 65/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0481 - val_mse: 0.0481\n",
      "Epoch 66/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0474 - val_mse: 0.0474\n",
      "Epoch 67/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0473 - val_mse: 0.0473\n",
      "Epoch 68/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0466 - val_mse: 0.0466\n",
      "Epoch 69/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0486 - val_mse: 0.0486\n",
      "Epoch 70/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0477 - val_mse: 0.0477\n",
      "Epoch 71/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0463 - val_mse: 0.0463\n",
      "Epoch 72/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0482 - val_mse: 0.0482\n",
      "Epoch 73/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0483 - val_mse: 0.0483\n",
      "Epoch 74/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0503 - val_mse: 0.0503\n",
      "Epoch 75/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0493 - val_mse: 0.0493\n",
      "Epoch 76/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0480 - val_mse: 0.0480\n",
      "Epoch 77/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0473 - val_mse: 0.0473\n",
      "Epoch 78/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0478 - val_mse: 0.0478\n",
      "Epoch 79/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0486 - val_mse: 0.0486\n",
      "Epoch 80/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0503 - val_mse: 0.0503\n",
      "Epoch 81/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0487 - val_mse: 0.0487\n",
      "Epoch 82/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0484 - val_mse: 0.0484\n",
      "Epoch 83/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0483 - val_mse: 0.0483\n",
      "Epoch 84/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0493 - val_mse: 0.0493\n",
      "Epoch 85/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0504 - val_mse: 0.0504\n",
      "Epoch 86/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0489 - val_mse: 0.0489\n",
      "Epoch 87/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0503 - val_mse: 0.0503\n",
      "Epoch 88/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0496 - val_mse: 0.0496\n",
      "Epoch 89/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0483 - val_mse: 0.0483\n",
      "Epoch 90/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0496 - val_mse: 0.0496\n",
      "Epoch 91/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0503 - val_mse: 0.0503\n",
      "Epoch 92/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0483 - val_mse: 0.0483\n",
      "Epoch 93/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0508 - val_mse: 0.0508\n",
      "Epoch 94/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0489 - val_mse: 0.0489\n",
      "Epoch 95/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0512 - val_mse: 0.0512\n",
      "Epoch 96/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0505 - val_mse: 0.0505\n",
      "Epoch 97/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0486 - val_mse: 0.0486\n",
      "Epoch 98/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0530 - val_mse: 0.0530\n",
      "Epoch 99/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0487 - val_mse: 0.0487\n",
      "Epoch 100/100\n",
      "488/488 [==============================] - 1s 1ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0509 - val_mse: 0.0509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.044209275"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dim = xtrain.shape[1]\n",
    "out_dim = ytrain.shape[1]\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"relu\"))\n",
    "model.add(Dense(2, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(out_dim))\n",
    "model.compile(loss=\"mse\", metrics = ['mse'], optimizer=\"adam\")\n",
    "\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs=100, batch_size=2, validation_split = 0.2, verbose=1)\n",
    "ypred = model.predict(xtest)\n",
    "\n",
    "score = tf.keras.metrics.mean_squared_error(\n",
    "    ytest, ypred\n",
    ")\n",
    "score= np.array(score)\n",
    "score.mean()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9205b650ad9b26ded1fb73bba57cf404cfc03cd0c8186ebe669ad7e6a2a6143"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ai4halth': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
