{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "autoenc_on_spectras_on_fixed_classes.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartynaPlomecka/AI4Health/blob/main/autoenc_on_spectras_on_fixed_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ar2a5mosbj8",
        "outputId": "2f65a833-f712-411d-acf8-cfef918b6f4a"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov  8 22:47:02 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kGYPyctspXw",
        "outputId": "76d42bc1-38e3-4e7c-af40-2f350c3caf3f"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiBt83nWX1ZY"
      },
      "source": [
        "import scipy.io\n",
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_theme(style=\"ticks\", color_codes=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import balanced_accuracy_score"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCMKijMGX1ZZ",
        "outputId": "4abb05dc-0591-45b5-bb63-8131f822a850"
      },
      "source": [
        "data = scipy.io.loadmat('x.mat')\n",
        "columns = np.asarray([['Electrode %d - %d/2 Hz'%(i+1, j+1)] for i in range(data['x'].shape[1]) for j in range(data['x'].shape[2])])\n",
        "data['x'].shape\n",
        "columns"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Electrode 1 - 1/2 Hz'],\n",
              "       ['Electrode 1 - 2/2 Hz'],\n",
              "       ['Electrode 1 - 3/2 Hz'],\n",
              "       ...,\n",
              "       ['Electrode 105 - 77/2 Hz'],\n",
              "       ['Electrode 105 - 78/2 Hz'],\n",
              "       ['Electrode 105 - 79/2 Hz']], dtype='<U23')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWA71-W3X1Za"
      },
      "source": [
        "labels = pd.read_csv(\"table_withlabels.csv\")\n",
        "foof = pd.read_csv(\"foof2features.csv\")\n",
        "beh = pd.read_csv(\"behaviorals.csv\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBoVPLD5X1Za"
      },
      "source": [
        "df = pd.DataFrame(data['x'].reshape((data['x'].shape[0], -1)))\n",
        "df.columns = columns\n",
        "df['IDs'] = foof['C1']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMnrjPZQX1Za",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "30e5b1c8-5cf2-4700-e250-b00184f768f4"
      },
      "source": [
        "df2 = pd.merge(df, labels[['label', 'IDs']], on='IDs', how='inner')\n",
        "df2"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(Electrode 1 - 1/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 2/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 3/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 4/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 5/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 6/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 7/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 8/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 9/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 10/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 11/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 12/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 13/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 14/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 15/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 16/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 17/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 18/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 19/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 20/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 21/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 22/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 23/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 24/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 25/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 26/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 27/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 28/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 29/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 30/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 31/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 32/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 33/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 34/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 35/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 36/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 37/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 38/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 39/2 Hz,)</th>\n",
              "      <th>(Electrode 1 - 40/2 Hz,)</th>\n",
              "      <th>...</th>\n",
              "      <th>(Electrode 105 - 42/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 43/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 44/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 45/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 46/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 47/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 48/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 49/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 50/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 51/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 52/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 53/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 54/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 55/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 56/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 57/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 58/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 59/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 60/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 61/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 62/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 63/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 64/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 65/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 66/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 67/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 68/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 69/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 70/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 71/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 72/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 73/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 74/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 75/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 76/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 77/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 78/2 Hz,)</th>\n",
              "      <th>(Electrode 105 - 79/2 Hz,)</th>\n",
              "      <th>IDs</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.707834e-12</td>\n",
              "      <td>1.512286e-10</td>\n",
              "      <td>2.475735e-09</td>\n",
              "      <td>3.453763e-08</td>\n",
              "      <td>4.554996e-07</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.001314</td>\n",
              "      <td>0.009992</td>\n",
              "      <td>0.045200</td>\n",
              "      <td>0.121630</td>\n",
              "      <td>0.205934</td>\n",
              "      <td>0.263955</td>\n",
              "      <td>0.354726</td>\n",
              "      <td>0.545805</td>\n",
              "      <td>0.795967</td>\n",
              "      <td>0.989854</td>\n",
              "      <td>1.027207</td>\n",
              "      <td>0.887269</td>\n",
              "      <td>0.637793</td>\n",
              "      <td>0.381529</td>\n",
              "      <td>0.189932</td>\n",
              "      <td>0.078685</td>\n",
              "      <td>2.712743e-02</td>\n",
              "      <td>7.783476e-03</td>\n",
              "      <td>1.862060e-03</td>\n",
              "      <td>3.942583e-04</td>\n",
              "      <td>1.977762e-04</td>\n",
              "      <td>6.257702e-04</td>\n",
              "      <td>2.305441e-03</td>\n",
              "      <td>7.111082e-03</td>\n",
              "      <td>1.813943e-02</td>\n",
              "      <td>3.825323e-02</td>\n",
              "      <td>6.669673e-02</td>\n",
              "      <td>0.096172</td>\n",
              "      <td>0.114774</td>\n",
              "      <td>0.113667</td>\n",
              "      <td>0.094283</td>\n",
              "      <td>0.067737</td>\n",
              "      <td>0.047193</td>\n",
              "      <td>...</td>\n",
              "      <td>4.607796e-02</td>\n",
              "      <td>3.274256e-02</td>\n",
              "      <td>2.406225e-02</td>\n",
              "      <td>1.763925e-02</td>\n",
              "      <td>1.271035e-02</td>\n",
              "      <td>8.968245e-03</td>\n",
              "      <td>6.191920e-03</td>\n",
              "      <td>4.182827e-03</td>\n",
              "      <td>2.764631e-03</td>\n",
              "      <td>1.787832e-03</td>\n",
              "      <td>1.131198e-03</td>\n",
              "      <td>7.002814e-04</td>\n",
              "      <td>4.241593e-04</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>8.488273e-07</td>\n",
              "      <td>4.044092e-07</td>\n",
              "      <td>1.885146e-07</td>\n",
              "      <td>8.597876e-08</td>\n",
              "      <td>3.836717e-08</td>\n",
              "      <td>1.675138e-08</td>\n",
              "      <td>7.155891e-09</td>\n",
              "      <td>2.990881e-09</td>\n",
              "      <td>1.223086e-09</td>\n",
              "      <td>4.893694e-10</td>\n",
              "      <td>1.915752e-10</td>\n",
              "      <td>7.337775e-11</td>\n",
              "      <td>2.749867e-11</td>\n",
              "      <td>1.008282e-11</td>\n",
              "      <td>3.617107e-12</td>\n",
              "      <td>1.269651e-12</td>\n",
              "      <td>NDARAA075AMK</td>\n",
              "      <td>No Diagnosis Given</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.798713e-02</td>\n",
              "      <td>1.639433e-01</td>\n",
              "      <td>2.897591e-01</td>\n",
              "      <td>2.097609e-01</td>\n",
              "      <td>6.347556e-02</td>\n",
              "      <td>0.027758</td>\n",
              "      <td>0.105298</td>\n",
              "      <td>0.184513</td>\n",
              "      <td>0.130638</td>\n",
              "      <td>0.163258</td>\n",
              "      <td>0.175982</td>\n",
              "      <td>0.191296</td>\n",
              "      <td>0.163549</td>\n",
              "      <td>0.165213</td>\n",
              "      <td>0.196133</td>\n",
              "      <td>0.183649</td>\n",
              "      <td>0.118244</td>\n",
              "      <td>0.051315</td>\n",
              "      <td>0.014980</td>\n",
              "      <td>0.002941</td>\n",
              "      <td>0.000388</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>8.280853e-08</td>\n",
              "      <td>2.879210e-09</td>\n",
              "      <td>3.045984e-08</td>\n",
              "      <td>1.168140e-06</td>\n",
              "      <td>2.824546e-05</td>\n",
              "      <td>4.166383e-04</td>\n",
              "      <td>3.732538e-03</td>\n",
              "      <td>2.029281e-02</td>\n",
              "      <td>6.694483e-02</td>\n",
              "      <td>1.340280e-01</td>\n",
              "      <td>1.629914e-01</td>\n",
              "      <td>0.121013</td>\n",
              "      <td>0.056922</td>\n",
              "      <td>0.022720</td>\n",
              "      <td>0.020140</td>\n",
              "      <td>0.034753</td>\n",
              "      <td>0.060380</td>\n",
              "      <td>...</td>\n",
              "      <td>2.199757e-01</td>\n",
              "      <td>2.263200e-01</td>\n",
              "      <td>2.260933e-01</td>\n",
              "      <td>2.196382e-01</td>\n",
              "      <td>2.081037e-01</td>\n",
              "      <td>1.944374e-01</td>\n",
              "      <td>2.205731e-01</td>\n",
              "      <td>2.351404e-01</td>\n",
              "      <td>1.587448e-01</td>\n",
              "      <td>1.481614e-01</td>\n",
              "      <td>1.462097e-01</td>\n",
              "      <td>1.457914e-01</td>\n",
              "      <td>1.438203e-01</td>\n",
              "      <td>0.137472</td>\n",
              "      <td>0.125241</td>\n",
              "      <td>0.107495</td>\n",
              "      <td>0.086270</td>\n",
              "      <td>0.064431</td>\n",
              "      <td>0.044651</td>\n",
              "      <td>0.028662</td>\n",
              "      <td>0.017027</td>\n",
              "      <td>0.009358</td>\n",
              "      <td>4.758187e-03</td>\n",
              "      <td>2.240159e-03</td>\n",
              "      <td>9.778570e-04</td>\n",
              "      <td>3.967077e-04</td>\n",
              "      <td>1.501681e-04</td>\n",
              "      <td>5.337634e-05</td>\n",
              "      <td>1.799164e-05</td>\n",
              "      <td>5.835291e-06</td>\n",
              "      <td>1.856717e-06</td>\n",
              "      <td>5.922864e-07</td>\n",
              "      <td>1.928109e-07</td>\n",
              "      <td>6.449918e-08</td>\n",
              "      <td>2.204650e-08</td>\n",
              "      <td>7.598678e-09</td>\n",
              "      <td>2.604260e-09</td>\n",
              "      <td>8.780452e-10</td>\n",
              "      <td>NDARAA112DMH</td>\n",
              "      <td>ADHD-Combined Type</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.104310e-06</td>\n",
              "      <td>4.299647e-05</td>\n",
              "      <td>5.644195e-04</td>\n",
              "      <td>4.760137e-03</td>\n",
              "      <td>2.579253e-02</td>\n",
              "      <td>0.089799</td>\n",
              "      <td>0.201005</td>\n",
              "      <td>0.290260</td>\n",
              "      <td>0.276177</td>\n",
              "      <td>0.196151</td>\n",
              "      <td>0.163322</td>\n",
              "      <td>0.211614</td>\n",
              "      <td>0.279784</td>\n",
              "      <td>0.325847</td>\n",
              "      <td>0.396573</td>\n",
              "      <td>0.559237</td>\n",
              "      <td>0.795523</td>\n",
              "      <td>0.994888</td>\n",
              "      <td>1.041194</td>\n",
              "      <td>0.901610</td>\n",
              "      <td>0.644717</td>\n",
              "      <td>0.380591</td>\n",
              "      <td>0.185469</td>\n",
              "      <td>7.461207e-02</td>\n",
              "      <td>2.477814e-02</td>\n",
              "      <td>6.792838e-03</td>\n",
              "      <td>1.537293e-03</td>\n",
              "      <td>2.872003e-04</td>\n",
              "      <td>4.429308e-05</td>\n",
              "      <td>5.639100e-06</td>\n",
              "      <td>5.926614e-07</td>\n",
              "      <td>5.145899e-08</td>\n",
              "      <td>9.058469e-09</td>\n",
              "      <td>3.938746e-07</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.003866</td>\n",
              "      <td>0.024211</td>\n",
              "      <td>0.081993</td>\n",
              "      <td>0.150162</td>\n",
              "      <td>...</td>\n",
              "      <td>2.535693e-01</td>\n",
              "      <td>2.297479e-01</td>\n",
              "      <td>2.023020e-01</td>\n",
              "      <td>1.731179e-01</td>\n",
              "      <td>1.439718e-01</td>\n",
              "      <td>1.163606e-01</td>\n",
              "      <td>9.139622e-02</td>\n",
              "      <td>6.976601e-02</td>\n",
              "      <td>5.175507e-02</td>\n",
              "      <td>3.731259e-02</td>\n",
              "      <td>2.614275e-02</td>\n",
              "      <td>1.780084e-02</td>\n",
              "      <td>1.177941e-02</td>\n",
              "      <td>0.007575</td>\n",
              "      <td>0.004734</td>\n",
              "      <td>0.002876</td>\n",
              "      <td>0.001697</td>\n",
              "      <td>0.000974</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>3.940847e-05</td>\n",
              "      <td>1.904584e-05</td>\n",
              "      <td>8.945487e-06</td>\n",
              "      <td>4.083206e-06</td>\n",
              "      <td>1.811307e-06</td>\n",
              "      <td>7.808657e-07</td>\n",
              "      <td>3.271553e-07</td>\n",
              "      <td>1.332064e-07</td>\n",
              "      <td>5.270957e-08</td>\n",
              "      <td>2.026971e-08</td>\n",
              "      <td>7.575282e-09</td>\n",
              "      <td>2.751336e-09</td>\n",
              "      <td>9.711398e-10</td>\n",
              "      <td>3.331297e-10</td>\n",
              "      <td>1.110552e-10</td>\n",
              "      <td>3.597966e-11</td>\n",
              "      <td>NDARAA117NEJ</td>\n",
              "      <td>ADHD-Combined Type</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.692639e-01</td>\n",
              "      <td>2.021357e-01</td>\n",
              "      <td>2.273063e-01</td>\n",
              "      <td>2.406964e-01</td>\n",
              "      <td>2.400039e-01</td>\n",
              "      <td>0.225430</td>\n",
              "      <td>0.202278</td>\n",
              "      <td>0.198154</td>\n",
              "      <td>0.228356</td>\n",
              "      <td>0.181836</td>\n",
              "      <td>0.088217</td>\n",
              "      <td>0.045228</td>\n",
              "      <td>0.026961</td>\n",
              "      <td>0.015631</td>\n",
              "      <td>0.008543</td>\n",
              "      <td>0.004406</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>0.010905</td>\n",
              "      <td>0.068405</td>\n",
              "      <td>0.165498</td>\n",
              "      <td>0.142894</td>\n",
              "      <td>0.043859</td>\n",
              "      <td>0.004787</td>\n",
              "      <td>1.876256e-04</td>\n",
              "      <td>3.305046e-06</td>\n",
              "      <td>2.458776e-07</td>\n",
              "      <td>9.796233e-08</td>\n",
              "      <td>8.917814e-08</td>\n",
              "      <td>1.360680e-07</td>\n",
              "      <td>2.324825e-07</td>\n",
              "      <td>4.000958e-07</td>\n",
              "      <td>6.824466e-07</td>\n",
              "      <td>1.151279e-06</td>\n",
              "      <td>1.920371e-06</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>0.010223</td>\n",
              "      <td>...</td>\n",
              "      <td>4.057367e-02</td>\n",
              "      <td>6.988496e-02</td>\n",
              "      <td>1.361531e-01</td>\n",
              "      <td>2.507349e-01</td>\n",
              "      <td>3.885431e-01</td>\n",
              "      <td>4.830309e-01</td>\n",
              "      <td>4.759660e-01</td>\n",
              "      <td>3.761277e-01</td>\n",
              "      <td>2.498527e-01</td>\n",
              "      <td>1.565416e-01</td>\n",
              "      <td>1.106800e-01</td>\n",
              "      <td>9.662594e-02</td>\n",
              "      <td>9.623675e-02</td>\n",
              "      <td>0.099740</td>\n",
              "      <td>0.103713</td>\n",
              "      <td>0.107273</td>\n",
              "      <td>0.110209</td>\n",
              "      <td>0.112444</td>\n",
              "      <td>0.113930</td>\n",
              "      <td>0.114638</td>\n",
              "      <td>0.114551</td>\n",
              "      <td>0.113672</td>\n",
              "      <td>1.120193e-01</td>\n",
              "      <td>1.096267e-01</td>\n",
              "      <td>1.065428e-01</td>\n",
              "      <td>1.028290e-01</td>\n",
              "      <td>9.855790e-02</td>\n",
              "      <td>9.381045e-02</td>\n",
              "      <td>8.867375e-02</td>\n",
              "      <td>8.323825e-02</td>\n",
              "      <td>7.759521e-02</td>\n",
              "      <td>7.183415e-02</td>\n",
              "      <td>6.604061e-02</td>\n",
              "      <td>6.029416e-02</td>\n",
              "      <td>5.466678e-02</td>\n",
              "      <td>4.922161e-02</td>\n",
              "      <td>4.401211e-02</td>\n",
              "      <td>3.908162e-02</td>\n",
              "      <td>NDARAA947ZG5</td>\n",
              "      <td>ADHD-Combined Type</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.332328e-06</td>\n",
              "      <td>1.552379e-04</td>\n",
              "      <td>1.804338e-03</td>\n",
              "      <td>1.287033e-02</td>\n",
              "      <td>5.615280e-02</td>\n",
              "      <td>0.149809</td>\n",
              "      <td>0.244887</td>\n",
              "      <td>0.247738</td>\n",
              "      <td>0.163687</td>\n",
              "      <td>0.094061</td>\n",
              "      <td>0.092551</td>\n",
              "      <td>0.144761</td>\n",
              "      <td>0.226166</td>\n",
              "      <td>0.316721</td>\n",
              "      <td>0.393643</td>\n",
              "      <td>0.433973</td>\n",
              "      <td>0.424374</td>\n",
              "      <td>0.368102</td>\n",
              "      <td>0.283226</td>\n",
              "      <td>0.193329</td>\n",
              "      <td>0.117118</td>\n",
              "      <td>0.063057</td>\n",
              "      <td>0.030347</td>\n",
              "      <td>1.337388e-02</td>\n",
              "      <td>5.954450e-03</td>\n",
              "      <td>3.541515e-03</td>\n",
              "      <td>3.564981e-03</td>\n",
              "      <td>4.895683e-03</td>\n",
              "      <td>7.210197e-03</td>\n",
              "      <td>1.056224e-02</td>\n",
              "      <td>1.516197e-02</td>\n",
              "      <td>2.127526e-02</td>\n",
              "      <td>2.917134e-02</td>\n",
              "      <td>3.908225e-02</td>\n",
              "      <td>0.051161</td>\n",
              "      <td>0.065440</td>\n",
              "      <td>0.081787</td>\n",
              "      <td>0.099876</td>\n",
              "      <td>0.119173</td>\n",
              "      <td>0.138942</td>\n",
              "      <td>...</td>\n",
              "      <td>3.532555e-08</td>\n",
              "      <td>9.568645e-09</td>\n",
              "      <td>2.467835e-09</td>\n",
              "      <td>6.060183e-10</td>\n",
              "      <td>1.416967e-10</td>\n",
              "      <td>3.154543e-11</td>\n",
              "      <td>6.686873e-12</td>\n",
              "      <td>1.349587e-12</td>\n",
              "      <td>2.593481e-13</td>\n",
              "      <td>4.751755e-14</td>\n",
              "      <td>8.215650e-15</td>\n",
              "      <td>1.332268e-15</td>\n",
              "      <td>2.220446e-16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>NDARAA948VFH</td>\n",
              "      <td>ADHD-Combined Type</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1780</th>\n",
              "      <td>3.397339e-06</td>\n",
              "      <td>1.701445e-05</td>\n",
              "      <td>7.576350e-05</td>\n",
              "      <td>2.999317e-04</td>\n",
              "      <td>1.055570e-03</td>\n",
              "      <td>0.003303</td>\n",
              "      <td>0.009185</td>\n",
              "      <td>0.022711</td>\n",
              "      <td>0.049919</td>\n",
              "      <td>0.097542</td>\n",
              "      <td>0.169458</td>\n",
              "      <td>0.261979</td>\n",
              "      <td>0.362192</td>\n",
              "      <td>0.457140</td>\n",
              "      <td>0.558556</td>\n",
              "      <td>0.718848</td>\n",
              "      <td>0.973054</td>\n",
              "      <td>1.218445</td>\n",
              "      <td>1.238427</td>\n",
              "      <td>0.952606</td>\n",
              "      <td>0.540327</td>\n",
              "      <td>0.226917</td>\n",
              "      <td>0.075227</td>\n",
              "      <td>2.648943e-02</td>\n",
              "      <td>1.792458e-02</td>\n",
              "      <td>2.097072e-02</td>\n",
              "      <td>2.761686e-02</td>\n",
              "      <td>3.624735e-02</td>\n",
              "      <td>4.652256e-02</td>\n",
              "      <td>5.821563e-02</td>\n",
              "      <td>7.098786e-02</td>\n",
              "      <td>8.434518e-02</td>\n",
              "      <td>9.764782e-02</td>\n",
              "      <td>1.101514e-01</td>\n",
              "      <td>0.121072</td>\n",
              "      <td>0.129664</td>\n",
              "      <td>0.135308</td>\n",
              "      <td>0.137579</td>\n",
              "      <td>0.136303</td>\n",
              "      <td>0.131578</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>NDARZN148PMN</td>\n",
              "      <td>Anxiety Disorders</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1781</th>\n",
              "      <td>8.241315e-06</td>\n",
              "      <td>2.938160e-04</td>\n",
              "      <td>4.641777e-03</td>\n",
              "      <td>3.249546e-02</td>\n",
              "      <td>1.008094e-01</td>\n",
              "      <td>0.138615</td>\n",
              "      <td>0.084707</td>\n",
              "      <td>0.024339</td>\n",
              "      <td>0.009157</td>\n",
              "      <td>0.021386</td>\n",
              "      <td>0.056133</td>\n",
              "      <td>0.119153</td>\n",
              "      <td>0.205499</td>\n",
              "      <td>0.296404</td>\n",
              "      <td>0.380990</td>\n",
              "      <td>0.479791</td>\n",
              "      <td>0.623989</td>\n",
              "      <td>0.793859</td>\n",
              "      <td>0.898112</td>\n",
              "      <td>0.847787</td>\n",
              "      <td>0.648699</td>\n",
              "      <td>0.398086</td>\n",
              "      <td>0.195731</td>\n",
              "      <td>7.824413e-02</td>\n",
              "      <td>2.775022e-02</td>\n",
              "      <td>1.265026e-02</td>\n",
              "      <td>1.178042e-02</td>\n",
              "      <td>1.622477e-02</td>\n",
              "      <td>2.354130e-02</td>\n",
              "      <td>3.351138e-02</td>\n",
              "      <td>4.641114e-02</td>\n",
              "      <td>6.248810e-02</td>\n",
              "      <td>8.179066e-02</td>\n",
              "      <td>1.040780e-01</td>\n",
              "      <td>0.128765</td>\n",
              "      <td>0.154909</td>\n",
              "      <td>0.181261</td>\n",
              "      <td>0.206372</td>\n",
              "      <td>0.228762</td>\n",
              "      <td>0.247127</td>\n",
              "      <td>...</td>\n",
              "      <td>3.941631e-01</td>\n",
              "      <td>4.002399e-01</td>\n",
              "      <td>4.017787e-01</td>\n",
              "      <td>3.987270e-01</td>\n",
              "      <td>3.911888e-01</td>\n",
              "      <td>3.794192e-01</td>\n",
              "      <td>3.638097e-01</td>\n",
              "      <td>3.448668e-01</td>\n",
              "      <td>3.231846e-01</td>\n",
              "      <td>2.994139e-01</td>\n",
              "      <td>2.742303e-01</td>\n",
              "      <td>2.483024e-01</td>\n",
              "      <td>2.222637e-01</td>\n",
              "      <td>0.196688</td>\n",
              "      <td>0.172072</td>\n",
              "      <td>0.148821</td>\n",
              "      <td>0.127245</td>\n",
              "      <td>0.107557</td>\n",
              "      <td>0.089879</td>\n",
              "      <td>0.074251</td>\n",
              "      <td>0.060641</td>\n",
              "      <td>0.048961</td>\n",
              "      <td>3.908051e-02</td>\n",
              "      <td>3.083834e-02</td>\n",
              "      <td>2.405713e-02</td>\n",
              "      <td>1.855320e-02</td>\n",
              "      <td>1.414542e-02</td>\n",
              "      <td>1.066191e-02</td>\n",
              "      <td>7.944675e-03</td>\n",
              "      <td>5.852474e-03</td>\n",
              "      <td>4.262113e-03</td>\n",
              "      <td>3.068545e-03</td>\n",
              "      <td>2.184048e-03</td>\n",
              "      <td>1.536788e-03</td>\n",
              "      <td>1.069025e-03</td>\n",
              "      <td>7.351630e-04</td>\n",
              "      <td>4.998062e-04</td>\n",
              "      <td>3.359246e-04</td>\n",
              "      <td>NDARZN277NR6</td>\n",
              "      <td>ADHD-Combined Type</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1782</th>\n",
              "      <td>5.472767e-03</td>\n",
              "      <td>8.816325e-03</td>\n",
              "      <td>1.629356e-02</td>\n",
              "      <td>3.226822e-02</td>\n",
              "      <td>6.212373e-02</td>\n",
              "      <td>0.108594</td>\n",
              "      <td>0.166461</td>\n",
              "      <td>0.220594</td>\n",
              "      <td>0.252412</td>\n",
              "      <td>0.252807</td>\n",
              "      <td>0.231613</td>\n",
              "      <td>0.215068</td>\n",
              "      <td>0.232839</td>\n",
              "      <td>0.303041</td>\n",
              "      <td>0.421870</td>\n",
              "      <td>0.561129</td>\n",
              "      <td>0.676698</td>\n",
              "      <td>0.727976</td>\n",
              "      <td>0.699085</td>\n",
              "      <td>0.607052</td>\n",
              "      <td>0.490319</td>\n",
              "      <td>0.386777</td>\n",
              "      <td>0.317491</td>\n",
              "      <td>2.844684e-01</td>\n",
              "      <td>2.785141e-01</td>\n",
              "      <td>2.883024e-01</td>\n",
              "      <td>3.053438e-01</td>\n",
              "      <td>3.247802e-01</td>\n",
              "      <td>3.442696e-01</td>\n",
              "      <td>3.627419e-01</td>\n",
              "      <td>3.796416e-01</td>\n",
              "      <td>3.945957e-01</td>\n",
              "      <td>4.073036e-01</td>\n",
              "      <td>4.175116e-01</td>\n",
              "      <td>0.425014</td>\n",
              "      <td>0.429657</td>\n",
              "      <td>0.431344</td>\n",
              "      <td>0.430042</td>\n",
              "      <td>0.425777</td>\n",
              "      <td>0.418636</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>NDARZN610GTY</td>\n",
              "      <td>Other Neurodevelopmental Disorders</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1783</th>\n",
              "      <td>2.658455e-03</td>\n",
              "      <td>5.749806e-03</td>\n",
              "      <td>1.180538e-02</td>\n",
              "      <td>2.300961e-02</td>\n",
              "      <td>4.257369e-02</td>\n",
              "      <td>0.074778</td>\n",
              "      <td>0.124685</td>\n",
              "      <td>0.197358</td>\n",
              "      <td>0.296551</td>\n",
              "      <td>0.423009</td>\n",
              "      <td>0.572803</td>\n",
              "      <td>0.736325</td>\n",
              "      <td>0.898562</td>\n",
              "      <td>1.041000</td>\n",
              "      <td>1.144986</td>\n",
              "      <td>1.195766</td>\n",
              "      <td>1.186043</td>\n",
              "      <td>1.117954</td>\n",
              "      <td>1.002806</td>\n",
              "      <td>0.858743</td>\n",
              "      <td>0.707077</td>\n",
              "      <td>0.568412</td>\n",
              "      <td>0.459428</td>\n",
              "      <td>3.907971e-01</td>\n",
              "      <td>3.662265e-01</td>\n",
              "      <td>3.824122e-01</td>\n",
              "      <td>4.298006e-01</td>\n",
              "      <td>4.942081e-01</td>\n",
              "      <td>5.593638e-01</td>\n",
              "      <td>6.101350e-01</td>\n",
              "      <td>6.357396e-01</td>\n",
              "      <td>6.319634e-01</td>\n",
              "      <td>6.015760e-01</td>\n",
              "      <td>5.527739e-01</td>\n",
              "      <td>0.496261</td>\n",
              "      <td>0.442074</td>\n",
              "      <td>0.397236</td>\n",
              "      <td>0.364814</td>\n",
              "      <td>0.344315</td>\n",
              "      <td>0.332907</td>\n",
              "      <td>...</td>\n",
              "      <td>5.303104e-01</td>\n",
              "      <td>5.122854e-01</td>\n",
              "      <td>5.034585e-01</td>\n",
              "      <td>5.008306e-01</td>\n",
              "      <td>5.006887e-01</td>\n",
              "      <td>4.992095e-01</td>\n",
              "      <td>4.930251e-01</td>\n",
              "      <td>4.796674e-01</td>\n",
              "      <td>4.578379e-01</td>\n",
              "      <td>4.274757e-01</td>\n",
              "      <td>3.896374e-01</td>\n",
              "      <td>3.462307e-01</td>\n",
              "      <td>2.996612e-01</td>\n",
              "      <td>0.252460</td>\n",
              "      <td>0.206958</td>\n",
              "      <td>0.165038</td>\n",
              "      <td>0.128005</td>\n",
              "      <td>0.096552</td>\n",
              "      <td>0.070820</td>\n",
              "      <td>0.050511</td>\n",
              "      <td>0.035031</td>\n",
              "      <td>0.023623</td>\n",
              "      <td>1.548895e-02</td>\n",
              "      <td>9.874659e-03</td>\n",
              "      <td>6.121086e-03</td>\n",
              "      <td>3.689262e-03</td>\n",
              "      <td>2.161992e-03</td>\n",
              "      <td>1.231889e-03</td>\n",
              "      <td>6.824829e-04</td>\n",
              "      <td>3.676327e-04</td>\n",
              "      <td>1.925477e-04</td>\n",
              "      <td>9.805386e-05</td>\n",
              "      <td>4.855040e-05</td>\n",
              "      <td>2.337344e-05</td>\n",
              "      <td>1.094093e-05</td>\n",
              "      <td>4.979521e-06</td>\n",
              "      <td>2.203549e-06</td>\n",
              "      <td>9.481114e-07</td>\n",
              "      <td>NDARZN677EYE</td>\n",
              "      <td>ADHD-Inattentive Type</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1784</th>\n",
              "      <td>4.856385e-04</td>\n",
              "      <td>3.048976e-03</td>\n",
              "      <td>1.546836e-02</td>\n",
              "      <td>5.645783e-02</td>\n",
              "      <td>1.419824e-01</td>\n",
              "      <td>0.245335</td>\n",
              "      <td>0.301591</td>\n",
              "      <td>0.291822</td>\n",
              "      <td>0.267454</td>\n",
              "      <td>0.265932</td>\n",
              "      <td>0.271648</td>\n",
              "      <td>0.258849</td>\n",
              "      <td>0.226517</td>\n",
              "      <td>0.199388</td>\n",
              "      <td>0.214474</td>\n",
              "      <td>0.297466</td>\n",
              "      <td>0.432314</td>\n",
              "      <td>0.551621</td>\n",
              "      <td>0.577562</td>\n",
              "      <td>0.487702</td>\n",
              "      <td>0.334765</td>\n",
              "      <td>0.200382</td>\n",
              "      <td>0.138771</td>\n",
              "      <td>1.460759e-01</td>\n",
              "      <td>1.628882e-01</td>\n",
              "      <td>1.428825e-01</td>\n",
              "      <td>1.077159e-01</td>\n",
              "      <td>9.204871e-02</td>\n",
              "      <td>9.582753e-02</td>\n",
              "      <td>1.062769e-01</td>\n",
              "      <td>1.169747e-01</td>\n",
              "      <td>1.261140e-01</td>\n",
              "      <td>1.330181e-01</td>\n",
              "      <td>1.372478e-01</td>\n",
              "      <td>0.138531</td>\n",
              "      <td>0.136790</td>\n",
              "      <td>0.132245</td>\n",
              "      <td>0.126957</td>\n",
              "      <td>0.130718</td>\n",
              "      <td>0.145625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>NDARZN899JCM</td>\n",
              "      <td>ADHD-Inattentive Type</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1785 rows × 8297 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      (Electrode 1 - 1/2 Hz,)  ...                               label\n",
              "0                7.707834e-12  ...                  No Diagnosis Given\n",
              "1                3.798713e-02  ...                  ADHD-Combined Type\n",
              "2                2.104310e-06  ...                  ADHD-Combined Type\n",
              "3                1.692639e-01  ...                  ADHD-Combined Type\n",
              "4                8.332328e-06  ...                  ADHD-Combined Type\n",
              "...                       ...  ...                                 ...\n",
              "1780             3.397339e-06  ...                   Anxiety Disorders\n",
              "1781             8.241315e-06  ...                  ADHD-Combined Type\n",
              "1782             5.472767e-03  ...  Other Neurodevelopmental Disorders\n",
              "1783             2.658455e-03  ...               ADHD-Inattentive Type\n",
              "1784             4.856385e-04  ...               ADHD-Inattentive Type\n",
              "\n",
              "[1785 rows x 8297 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9O2ukS9X1Zb",
        "outputId": "b03aeb00-7951-4609-b1ac-8f17508fedac"
      },
      "source": [
        "print(df2['label'].value_counts())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Other Neurodevelopmental Disorders    492\n",
            "ADHD-Inattentive Type                 388\n",
            "ADHD-Combined Type                    376\n",
            "Anxiety Disorders                     241\n",
            "No Diagnosis Given                    203\n",
            "Depressive Disorders                   85\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzcbk6N4Fi71",
        "outputId": "38cf4a42-04f4-407f-aca7-f75f87806f08"
      },
      "source": [
        "df2.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1785, 8297)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYmZ78tbX1Zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35eae41-8038-43bb-c599-d4f412095a2d"
      },
      "source": [
        "X = df2[df2.columns.difference(['IDs', 'label'])]\n",
        "y = df2['label']\n",
        "print(X.shape)\n",
        "y.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1785, 8295)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1785,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWQ6ALkzXd91"
      },
      "source": [
        "# number of input columns\n",
        "n_inputs = X.shape[1]\n",
        "# split into train test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# scale data\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kBu2xVt8XvMp",
        "outputId": "d2ec5044-7c1e-4544-aeec-15e3bf6cf27b"
      },
      "source": [
        "# define encoder\n",
        "visible = Input(shape=(n_inputs,))\n",
        "# encoder level 1\n",
        "e = Dense(n_inputs*2)(visible)\n",
        "e = BatchNormalization()(e)\n",
        "e = LeakyReLU()(e)\n",
        "# encoder level 2\n",
        "e = Dense(n_inputs)(e)\n",
        "e = BatchNormalization()(e)\n",
        "e = LeakyReLU()(e)\n",
        "# bottleneck\n",
        "n_bottleneck = round(float(n_inputs) / 5.0)\n",
        "bottleneck = Dense(n_bottleneck)(e)\n",
        "# define decoder, level 1\n",
        "d = Dense(n_inputs)(bottleneck)\n",
        "d = BatchNormalization()(d)\n",
        "d = LeakyReLU()(d)\n",
        "# decoder level 2\n",
        "d = Dense(n_inputs*2)(d)\n",
        "d = BatchNormalization()(d)\n",
        "d = LeakyReLU()(d)\n",
        "# output layer\n",
        "output = Dense(n_inputs, activation='linear')(d)\n",
        "# define autoencoder model\n",
        "model = Model(inputs=visible, outputs=output)\n",
        "# compile autoencoder model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# plot the autoencoder\n",
        "plot_model(model, 'autoencoder_compress.png', show_shapes=True)\n",
        "# fit the autoencoder model to reconstruct input\n",
        "history = model.fit(X_train, X_train, epochs=50, batch_size=16, verbose=2, validation_data=(X_test,X_test))\n",
        "# plot loss\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "# define an encoder model (without the decoder)\n",
        "encoder = Model(inputs=visible, outputs=bottleneck)\n",
        "plot_model(encoder, 'encoder_compress.png', show_shapes=True)\n",
        "# save the encoder to file\n",
        "encoder.save('encoder.h5')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "75/75 - 7s - loss: 0.4754 - val_loss: 0.8414\n",
            "Epoch 2/50\n",
            "75/75 - 5s - loss: 0.0617 - val_loss: 0.1130\n",
            "Epoch 3/50\n",
            "75/75 - 5s - loss: 0.0653 - val_loss: 0.2263\n",
            "Epoch 4/50\n",
            "75/75 - 5s - loss: 0.0846 - val_loss: 0.0700\n",
            "Epoch 5/50\n",
            "75/75 - 5s - loss: 0.0606 - val_loss: 0.0631\n",
            "Epoch 6/50\n",
            "75/75 - 5s - loss: 0.0700 - val_loss: 0.1272\n",
            "Epoch 7/50\n",
            "75/75 - 5s - loss: 0.2276 - val_loss: 3.1113\n",
            "Epoch 8/50\n",
            "75/75 - 5s - loss: 0.5505 - val_loss: 0.1788\n",
            "Epoch 9/50\n",
            "75/75 - 5s - loss: 0.0325 - val_loss: 0.0592\n",
            "Epoch 10/50\n",
            "75/75 - 5s - loss: 0.0270 - val_loss: 0.0268\n",
            "Epoch 11/50\n",
            "75/75 - 5s - loss: 0.0295 - val_loss: 0.0646\n",
            "Epoch 12/50\n",
            "75/75 - 5s - loss: 0.0315 - val_loss: 0.0345\n",
            "Epoch 13/50\n",
            "75/75 - 5s - loss: 0.0305 - val_loss: 0.0263\n",
            "Epoch 14/50\n",
            "75/75 - 5s - loss: 0.0261 - val_loss: 0.0428\n",
            "Epoch 15/50\n",
            "75/75 - 5s - loss: 0.0306 - val_loss: 0.0282\n",
            "Epoch 16/50\n",
            "75/75 - 5s - loss: 0.0358 - val_loss: 0.0601\n",
            "Epoch 17/50\n",
            "75/75 - 5s - loss: 0.0362 - val_loss: 0.0360\n",
            "Epoch 18/50\n",
            "75/75 - 5s - loss: 0.0326 - val_loss: 0.0380\n",
            "Epoch 19/50\n",
            "75/75 - 5s - loss: 0.0371 - val_loss: 0.0432\n",
            "Epoch 20/50\n",
            "75/75 - 5s - loss: 0.0340 - val_loss: 0.0371\n",
            "Epoch 21/50\n",
            "75/75 - 5s - loss: 0.0435 - val_loss: 0.0234\n",
            "Epoch 22/50\n",
            "75/75 - 5s - loss: 0.0327 - val_loss: 0.0447\n",
            "Epoch 23/50\n",
            "75/75 - 5s - loss: 0.0331 - val_loss: 0.0326\n",
            "Epoch 24/50\n",
            "75/75 - 5s - loss: 0.0361 - val_loss: 0.0835\n",
            "Epoch 25/50\n",
            "75/75 - 5s - loss: 0.0353 - val_loss: 0.0566\n",
            "Epoch 26/50\n",
            "75/75 - 5s - loss: 0.0288 - val_loss: 0.0549\n",
            "Epoch 27/50\n",
            "75/75 - 5s - loss: 0.0317 - val_loss: 0.0372\n",
            "Epoch 28/50\n",
            "75/75 - 5s - loss: 0.0347 - val_loss: 0.0394\n",
            "Epoch 29/50\n",
            "75/75 - 5s - loss: 0.0282 - val_loss: 0.0343\n",
            "Epoch 30/50\n",
            "75/75 - 5s - loss: 0.0273 - val_loss: 0.0365\n",
            "Epoch 31/50\n",
            "75/75 - 5s - loss: 0.0289 - val_loss: 0.0372\n",
            "Epoch 32/50\n",
            "75/75 - 5s - loss: 0.0260 - val_loss: 0.0559\n",
            "Epoch 33/50\n",
            "75/75 - 5s - loss: 0.0286 - val_loss: 0.1058\n",
            "Epoch 34/50\n",
            "75/75 - 5s - loss: 0.0288 - val_loss: 0.0524\n",
            "Epoch 35/50\n",
            "75/75 - 5s - loss: 0.0292 - val_loss: 0.0333\n",
            "Epoch 36/50\n",
            "75/75 - 5s - loss: 0.0266 - val_loss: 0.0355\n",
            "Epoch 37/50\n",
            "75/75 - 5s - loss: 0.0272 - val_loss: 0.0366\n",
            "Epoch 38/50\n",
            "75/75 - 5s - loss: 0.0268 - val_loss: 0.0348\n",
            "Epoch 39/50\n",
            "75/75 - 5s - loss: 0.0281 - val_loss: 0.0284\n",
            "Epoch 40/50\n",
            "75/75 - 5s - loss: 0.0283 - val_loss: 0.0355\n",
            "Epoch 41/50\n",
            "75/75 - 5s - loss: 0.0269 - val_loss: 0.0405\n",
            "Epoch 42/50\n",
            "75/75 - 5s - loss: 0.0271 - val_loss: 0.0481\n",
            "Epoch 43/50\n",
            "75/75 - 5s - loss: 0.0270 - val_loss: 0.0380\n",
            "Epoch 44/50\n",
            "75/75 - 5s - loss: 0.0254 - val_loss: 0.0561\n",
            "Epoch 45/50\n",
            "75/75 - 5s - loss: 0.0267 - val_loss: 0.0341\n",
            "Epoch 46/50\n",
            "75/75 - 5s - loss: 0.0250 - val_loss: 0.0372\n",
            "Epoch 47/50\n",
            "75/75 - 5s - loss: 0.0256 - val_loss: 0.0351\n",
            "Epoch 48/50\n",
            "75/75 - 5s - loss: 0.0276 - val_loss: 0.0763\n",
            "Epoch 49/50\n",
            "75/75 - 5s - loss: 0.0247 - val_loss: 0.0642\n",
            "Epoch 50/50\n",
            "75/75 - 5s - loss: 0.0218 - val_loss: 0.0353\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "wJBGZ06ZlbVS",
        "outputId": "6a0ae41d-7559-4760-8b4f-09074cdb129f"
      },
      "source": [
        "# define model 1\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=10000, random_state=0)\n",
        "# fit model on training set\n",
        "model.fit(X_train, y_train)\n",
        "# make prediction on test set\n",
        "yhat = model.predict(X_test)\n",
        "# calculate accuracy\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print(acc)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-d1f972d9dedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fit model on training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# make prediction on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1599\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1601\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m                 \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"iprint\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"maxiter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m             )\n\u001b[1;32m    938\u001b[0m             n_iter_i = _check_optimize_result(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, *args)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_multi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_multinomial_loss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss_grad\u001b[0;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),\n\u001b[1;32m    348\u001b[0m                     dtype=X.dtype)\n\u001b[0;32m--> 349\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss\u001b[0;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mintercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGd2GEwNmDoV",
        "outputId": "cd45ca1f-2074-4087-9ac9-3446c75f108b"
      },
      "source": [
        "model2 = RandomForestClassifier(n_estimators=1000, class_weight='balanced')\n",
        "model2.fit(X_train, y_train)\n",
        "preds = model2.predict(X_test)\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print(acc)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.18983050847457628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIsU47QTm1fa"
      },
      "source": [
        "# evaluate logistic regression on encoded input\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "# define dataset\n",
        "# split into train test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# scale data\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)\n",
        "# load the model from file\n",
        "encoder= load_model('encoder.h5')\n",
        "# encode the train data\n",
        "X_train_encode = encoder.predict(X_train)\n",
        "# encode the test data\n",
        "X_test_encode = encoder.predict(X_test)\n",
        "# define the model\n",
        "model = svm.SVC(decision_function_shape='ovo')\n",
        "# fit the model on the training set\n",
        "model.fit(X_train_encode, y_train)\n",
        "# make predictions on the test set\n",
        "yhat = model.predict(X_test_encode)\n",
        "# calculate classification accuracy\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}