{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "import pandas\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2042, 105, 79)\n"
     ]
    }
   ],
   "source": [
    "data = scipy.io.loadmat('data/x.mat')  \n",
    "print(data['x'].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Electrode 1 - 1/2 Hz'],\n",
       "       ['Electrode 1 - 2/2 Hz'],\n",
       "       ['Electrode 1 - 3/2 Hz'],\n",
       "       ...,\n",
       "       ['Electrode 105 - 77/2 Hz'],\n",
       "       ['Electrode 105 - 78/2 Hz'],\n",
       "       ['Electrode 105 - 79/2 Hz']], dtype='<U23')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = np.asarray([['Electrode %d - %d/2 Hz'%(i+1, j+1)] for i in range(data['x'].shape[1]) for j in range(data['x'].shape[2])])\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"data/table_withlabels.csv\")\n",
    "foof = pd.read_csv(\"data/foof2features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other Neurodevelopmental Disorders    492\n",
      "ADHD-Inattentive Type                 388\n",
      "ADHD-Combined Type                    376\n",
      "Anxiety Disorders                     241\n",
      "No Diagnosis Given                    203\n",
      "Depressive Disorders                   85\n",
      "Name: label, dtype: int64\n",
      "(1785, 8297)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data['x'].reshape((data['x'].shape[0], -1)))\n",
    "df.columns = columns\n",
    "df['IDs'] = foof['C1']\n",
    "df = pd.merge(df, labels[['label', 'IDs']], on='IDs', how='inner')\n",
    "print(df['label'].value_counts())\n",
    "dataset = df.values\n",
    "print(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1785, 8295)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset[:,0:8295].astype(float)\n",
    "y = dataset[:,8296]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "#X = np.clip(X, -1, 1)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anujanegi/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4 5] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2064, 8295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anujanegi/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4 5] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(888, 8295)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, stratify=y,test_size=0.3)\n",
    "\n",
    "oversample = SMOTE() #oversample to aaccount for the data imbalance\n",
    "trainX,trainy = oversample.fit_resample(trainX,trainy)\n",
    "testX,testy = oversample.fit_resample(testX,testy)\n",
    "\n",
    "print(trainX.shape)\n",
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainX = trainX.reshape((2064,105,79)) \n",
    "testX = testX.reshape((888,105,79)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def evaluate(y_test, y_pred, show_cm=False):\n",
    "    print(y_test)\n",
    "    print(y_pred)\n",
    "    y_test = np.argmax(y_test, axis=1) # assuming you have n-by-6 class_prob\n",
    "    y_pred = np.argmax(y_pred, axis=1) # assuming you have n-by-6 class_prob\n",
    "    print(y_test)\n",
    "    print(y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(y_test, y_pred, average='macro'))\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred, average='macro'))\n",
    "#     print(\"ROC AUC:\", metrics.roc_auc_score(y_test, y_pred, multi_class='ovo',))\n",
    "    print(\"F1 score:\", metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "#     print(\"Brier Score:\", metrics.brier_score_loss(y_test, y_pred)) # only for binary classification\n",
    "    labels = ['Other Neurodevelopmental Disorders', 'ADHD-Inattentive Type', 'ADHD-Combined Type', 'Anxiety Disorders', 'No Diagnosis Given', 'Depressive Disorders']\n",
    "    if show_cm:\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        disp.plot()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.transpose(trainX, (0, 2,1))\n",
    "testX = np.transpose(testX, (0, 2,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print('Tensorflow Version: ',tensorflow.__version__)\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 79, 32)            13472     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 39, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 39, 64)            6208      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 39, 64)           256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 19, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 19, 128)           24704     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 19, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 9, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 9, 256)            98560     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 9, 256)           1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 4, 256)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 4, 512)            393728    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 4, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 2, 512)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 2, 1024)           1573888   \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 1, 1024)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 6150      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,120,550\n",
      "Trainable params: 2,118,630\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "# Convolution\n",
    "model.add(Conv1D(filters = 32, kernel_size =4, input_shape = (79, 105), activation = 'relu', padding = 'same'))\n",
    "#model.add(Conv1D(filters = 32, kernel_size = 5, input_shape = (79, 32), activation = 'relu', padding = 'same'))\n",
    "# Pooling\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "# Convolution\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, input_shape = (39, 32), activation = 'relu', padding = 'same'))\n",
    "##model.add(Conv1D(filters = 64, kernel_size = 3, input_shape = (39, 64), activation = 'relu', padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Conv1D(filters =128, kernel_size = 3, input_shape = (19, 64), activation = 'relu', padding = 'same'))\n",
    "##model.add(Conv1D(filters =128, kernel_size = 3, input_shape = (19, 128), activation = 'relu', padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Conv1D(filters =256, kernel_size = 3, input_shape = (9, 128), activation = 'relu', padding = 'same'))\n",
    "#model.add(Conv1D(filters =256, kernel_size = 3, input_shape = (9, 256), activation = 'relu', padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Conv1D(filters =512, kernel_size = 3, input_shape = (4, 256), activation = 'relu', padding = 'same'))\n",
    "#model.add(Conv1D(filters =512, kernel_size = 3, input_shape = (4, 512), activation = 'relu', padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Conv1D(filters = 1024, kernel_size = 3, input_shape = (2, 512), activation = 'relu', padding = 'same'))\n",
    "#model.add(Conv1D(filters = 1024, kernel_size = 3, input_shape = (2, 1028), activation = 'relu', padding = 'same'))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "# Full connection\n",
    "#model.add(Dense(units = 512, activation = 'relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(units = 128, activation = 'relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(units = 32, activation = 'relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 6, activation = 'softmax'))\n",
    "# Compiling the CNN\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_6 (Conv1D)           (None, 79, 32)            10112     \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 39, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 39, 64)            6208      \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 39, 64)            12352     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 39, 64)           256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 19, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 19, 128)           24704     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 19, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPooling  (None, 9, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 9, 256)            98560     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 9, 256)           1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPooling  (None, 4, 256)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 4, 512)            393728    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 4, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPoolin  (None, 2, 512)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 2, 1024)           1573888   \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 2, 1024)           3146752   \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPoolin  (None, 1, 1024)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 6150      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,276,294\n",
      "Trainable params: 5,274,374\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "# Convolution\n",
    "model.add(Conv1D(filters = 32, kernel_size = 3, input_shape = (79, 105), activation = 'relu', padding = 'same'))\n",
    "# Pooling\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "# Convolution\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, input_shape = (39, 32), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, input_shape = (39, 64), activation = 'relu', padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Conv1D(filters =128, kernel_size = 3, input_shape = (19, 64), activation = 'relu', padding = 'same'))\n",
    "##model.add(Conv1D(filters =128, kearnel_size = 3, input_shape = (19, 128), activation = 'relu', padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Conv1D(filters =256, kernel_size = 3, input_shape = (9, 128), activation = 'relu', padding = 'same'))\n",
    "#model.add(Conv1D(filters =256, kernel_size = 3, input_shape = (9, 256), activation = 'relu', padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Conv1D(filters =512, kernel_size = 3, input_shape = (4, 256), activation = 'relu', padding = 'same'))\n",
    "#model.add(Conv1D(filters =512, kernel_size = 3, input_shape = (4, 512), activation = 'relu', padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Conv1D(filters = 1024, kernel_size = 3, input_shape = (2, 512), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv1D(filters = 1024, kernel_size = 3, input_shape = (2, 1028), activation = 'relu', padding = 'same'))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units = 6, activation = 'softmax'))\n",
    "# Compiling the CNN\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "52/52 [==============================] - 14s 264ms/step - loss: 1.2679 - accuracy: 0.4949 - val_loss: 1.9307 - val_accuracy: 0.2038\n",
      "Epoch 2/2\n",
      "52/52 [==============================] - 14s 260ms/step - loss: 1.0729 - accuracy: 0.5851 - val_loss: 2.3679 - val_accuracy: 0.2083\n",
      "[[1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]]\n",
      "[[6.90738335e-02 6.53154373e-01 4.88215350e-02 2.96107922e-02\n",
      "  4.29033861e-02 1.56435996e-01]\n",
      " [1.76301375e-01 3.12826216e-01 6.01986982e-02 6.90569170e-03\n",
      "  3.82769145e-02 4.05491084e-01]\n",
      " [5.47418356e-01 1.83356762e-01 5.75538166e-02 4.39276081e-03\n",
      "  4.37455140e-02 1.63532868e-01]\n",
      " ...\n",
      " [1.42905414e-01 5.40908873e-01 5.53859212e-02 2.59473752e-02\n",
      "  5.79080172e-02 1.76944479e-01]\n",
      " [7.84091473e-01 1.16123036e-02 1.09364770e-01 3.32288851e-04\n",
      "  7.29466137e-03 8.73045027e-02]\n",
      " [6.68763638e-01 1.04118735e-01 2.83516310e-02 7.57545419e-03\n",
      "  8.57784525e-02 1.05412178e-01]]\n",
      "[0 0 1 4 3 5 1 1 5 1 1 5 4 0 1 5 0 5 1 5 1 2 0 5 5 2 1 5 5 5 5 4 4 5 0 0 5\n",
      " 1 2 0 1 0 2 5 5 5 5 1 4 2 1 2 5 0 0 0 2 1 5 1 5 0 2 5 0 2 2 5 2 0 1 5 2 4\n",
      " 5 1 5 1 5 2 1 5 3 4 4 4 2 2 1 1 0 1 3 5 2 5 0 0 4 0 0 5 5 5 5 5 0 3 4 5 2\n",
      " 1 0 4 0 2 1 2 1 5 4 0 5 5 2 5 0 0 0 2 0 1 4 5 5 1 0 5 0 5 2 1 0 2 1 2 1 4\n",
      " 3 0 2 5 5 2 3 5 1 1 0 1 1 4 4 2 3 2 3 3 0 3 5 1 0 1 5 1 5 4 5 5 0 2 2 2 5\n",
      " 5 1 1 5 3 0 5 1 1 1 0 5 0 4 5 0 0 3 5 0 1 0 1 2 0 1 2 0 0 5 1 1 0 1 5 2 2\n",
      " 4 3 5 1 1 3 4 5 5 1 0 5 4 4 5 2 4 2 5 0 0 4 1 4 1 4 1 1 5 2 0 2 5 0 0 0 4\n",
      " 1 0 0 4 2 1 5 5 5 0 2 4 3 2 5 1 0 3 1 5 1 0 1 1 2 2 5 5 0 0 4 1 4 2 1 5 1\n",
      " 0 0 1 0 5 1 0 0 5 5 4 5 5 0 4 2 2 0 0 5 3 5 2 5 2 0 0 5 0 1 4 5 1 5 0 2 5\n",
      " 4 0 0 0 1 5 0 1 2 3 1 5 2 5 2 0 5 2 1 4 1 5 1 0 5 5 5 1 1 1 1 0 2 5 1 5 5\n",
      " 4 5 5 2 0 2 4 0 5 5 4 4 1 2 2 5 0 5 0 0 1 5 5 4 5 4 0 2 1 1 5 1 5 5 5 4 5\n",
      " 1 1 0 0 5 1 2 1 4 0 2 1 3 1 3 0 4 1 2 1 5 0 0 1 1 5 0 2 0 5 1 0 4 5 3 0 5\n",
      " 1 4 4 3 1 4 5 5 5 2 2 5 0 5 0 0 1 3 1 1 3 0 1 1 5 5 5 5 1 1 0 2 1 5 2 0 4\n",
      " 0 5 1 0 1 5 1 5 5 5 0 1 5 2 4 5 5 5 4 4 2 5 2 4 4 0 5 3 1 2 0 0 5 5 1 0 1\n",
      " 1 0 4 0 4 0 4 4 1 1 5 5 5 4 5 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[1 5 0 5 0 0 1 5 0 0 1 5 2 3 1 0 4 1 5 1 2 2 5 0 2 1 1 0 1 5 1 1 1 1 5 1 1\n",
      " 5 1 1 0 0 5 4 1 0 5 1 0 1 0 1 0 1 5 1 1 1 2 4 5 4 1 0 1 5 1 1 2 1 2 3 1 1\n",
      " 0 1 5 4 4 0 1 2 1 1 0 0 0 1 0 1 0 1 0 0 2 0 5 1 0 4 0 0 5 1 1 1 1 0 5 1 0\n",
      " 0 5 5 1 1 1 1 1 1 1 1 0 1 1 1 1 5 5 2 4 1 0 2 1 0 0 1 0 1 2 0 0 1 1 4 0 1\n",
      " 5 1 1 5 5 0 5 4 1 0 0 2 1 3 1 1 1 1 1 5 1 3 4 2 0 1 0 0 4 2 0 1 1 4 1 2 1\n",
      " 0 1 1 0 2 3 2 4 2 0 0 4 1 1 1 1 1 2 1 1 0 1 1 0 0 5 1 1 5 1 5 1 1 5 0 5 1\n",
      " 1 0 1 1 1 1 1 3 1 1 1 5 0 1 1 0 5 0 1 1 1 2 0 0 0 0 2 2 5 4 4 1 5 2 2 5 5\n",
      " 0 2 4 5 1 5 5 1 1 5 5 0 1 1 0 1 0 3 3 1 0 1 5 1 1 1 0 2 1 0 2 1 0 1 5 2 0\n",
      " 2 4 5 1 1 1 1 0 1 1 1 1 0 1 0 1 5 1 0 0 4 2 0 1 1 5 2 1 0 1 0 0 1 1 0 5 0\n",
      " 1 2 5 4 2 5 0 0 2 5 1 2 4 0 5 1 1 1 0 1 0 5 0 1 3 0 1 1 2 0 1 0 4 0 1 4 1\n",
      " 2 1 0 1 2 5 2 0 2 4 1 5 1 1 2 5 1 5 2 1 0 4 5 1 5 0 0 1 1 0 4 1 2 1 0 1 1\n",
      " 1 0 1 0 0 2 3 5 4 0 0 1 1 2 2 1 5 4 3 1 1 1 5 4 2 1 1 1 5 0 1 1 4 0 5 1 1\n",
      " 1 0 4 1 1 3 4 5 0 2 1 4 0 2 0 0 1 1 0 1 1 2 0 1 1 1 2 1 4 0 1 2 5 1 4 1 1\n",
      " 1 0 1 0 1 1 1 1 1 0 0 1 1 3 1 1 0 3 2 1 1 1 5 2 1 0 1 0 1 4 0 1 0 2 5 2 1\n",
      " 1 0 0 0 2 1 1 0 1 2 2 5 1 0 1 4 1 5 2 5 5 0 5 1 1 2 1 0 1 0 3 1 5 1 0 1 1\n",
      " 0 0 2 0 2 4 5 3 0 1 5 1 0 5 2 0 2 0 1 1 1 0 1 3 0 1 1 1 3 2 0 1 0 0 1 2 1\n",
      " 2 0 1 1 2 0 3 4 2 1 0 1 0 1 2 2 1 0 1 5 1 2 2 2 0 1 2 0 2 0 2 2 1 2 4 1 1\n",
      " 1 1 1 1 1 1 5 2 1 4 3 4 0 0 0 1 2 3 3 1 1 2 2 1 5 1 4 0 1 0 2 1 2 1 1 1 2\n",
      " 1 2 1 1 1 1 1 2 2 4 0 1 1 0 2 1 1 5 5 1 2 5 1 3 2 0 5 1 5 5 1 0 1 0 2 3 5\n",
      " 1 3 5 5 3 2 2 4 0 3 3 2 5 5 1 0 2 0 3 1 1 3 1 0 2 5 3 2 5 3 4 1 0 5 3 3 1\n",
      " 2 1 3 5 1 2 5 1 1 3 1 1 0 1 2 5 1 2 2 0 2 1 3 2 2 3 1 0 0 0 1 0 2 0 1 3 3\n",
      " 3 2 2 1 1 0 2 0 0 0 1 5 3 4 0 0 1 2 1 0 5 3 0 0 2 1 2 1 1 5 1 1 1 1 2 5 2\n",
      " 0 1 0 1 0 0 0 1 2 3 1 1 1 2 1 3 0 2 1 2 1 1 5 2 1 2 1 5 1 1 0 1 3 5 1 1 2\n",
      " 5 3 2 0 1 2 1 1 1 1 4 2 1 2 1 2 2 0 0 1 5 3 1 3 1 5 2 2 3 1 1 0 1 3 1 0 0]\n",
      "Accuracy: 0.20833333333333334\n",
      "Precision: 0.22390483408838122\n",
      "Recall: 0.20833333333333334\n",
      "F1 score: 0.19140786913241004\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(trainX, trainy, validation_split=0.2, epochs=2, verbose=1)\n",
    "# evaluate the model\n",
    "evaluate(testy, model.predict(testX))#, show_cm=True)\n",
    "#evaluate(trainy, model.predict(trainX))#, show_cm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.90738335e-02, 6.53154373e-01, 4.88215350e-02, 2.96107922e-02,\n",
       "        4.29033861e-02, 1.56435996e-01],\n",
       "       [1.76301375e-01, 3.12826216e-01, 6.01986982e-02, 6.90569170e-03,\n",
       "        3.82769145e-02, 4.05491084e-01],\n",
       "       [5.47418356e-01, 1.83356762e-01, 5.75538166e-02, 4.39276081e-03,\n",
       "        4.37455140e-02, 1.63532868e-01],\n",
       "       ...,\n",
       "       [1.42905414e-01, 5.40908873e-01, 5.53859212e-02, 2.59473752e-02,\n",
       "        5.79080172e-02, 1.76944479e-01],\n",
       "       [7.84091473e-01, 1.16123036e-02, 1.09364770e-01, 3.32288851e-04,\n",
       "        7.29466137e-03, 8.73045027e-02],\n",
       "       [6.68763638e-01, 1.04118735e-01, 2.83516310e-02, 7.57545419e-03,\n",
       "        8.57784525e-02, 1.05412178e-01]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(testX)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9205b650ad9b26ded1fb73bba57cf404cfc03cd0c8186ebe669ad7e6a2a6143"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
